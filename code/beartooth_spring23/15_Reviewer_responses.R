## Recovery of Stream Productivity following Disturbance
## Originally created: July 10, 2023
## Heili Lowman

#### READ ME ####

# The following set of scripts will walk through the steps necessary to
# prep and send data to Beartooth as well as process the model outputs.

# Much of this code has been modified from the RiverBiomass repository
# found at: https://github.com/jrblaszczak/RiverBiomass 

# Please note, the "data_XX" folders have been ignored using git.ignore.
# Links to the raw data sets are provided in the 01_NWIS_RiverSelection.R file.

# If you are accessing the code via GitHub, these will need to be 
# downloaded and added to a folder of the appropriate name prior to running the 
# code below.

# This file will address remaining analyses to support responses to reviewer
# comments.

# The following script was run using the Pinyon server at the University of 
# Nevada Reno for speed when re-running the full model at 6 selected sites.

#### Setup ####

# Load necessary packages.
lapply(c("tidybayes", "brms", "tidyverse", "lubridate", 
         "data.table", "GGally", "plotly", "bayesplot",
         "multcomp", "patchwork", "bayesplot", "shinystan",
         "modelsummary", "here", "nlme", "loo", "parallel",
         "tidyverse", "rstan", "devtools", "moments"), 
       require, character.only=T)

#### Data ####

# Import necessary datasets.

# First, the model outputs.
dat_out <- readRDS("data_working/beartooth_181rivers_model_params_all_iterations_050823.rds")

# And, the original data used in modeling.
dat_in <- readRDS("data_working/df_181sites_Qmaxnorm_SavoySL.rds")

# Then, the data for the maximum accrual (amax) models.
dat_amax <- readRDS("data_working/amax_covariates_143sites_110723.rds")

# Next, the data for the Qc:Q2yr models.
dat_Qc <- readRDS("data_working/Qc_covariates_130sites_110723.rds") %>%
  mutate(Qc_Q2yr = Qc/RI_2yr_Q_cms)

# And Qc data at *all* sites.
dat_Qc_all <- readRDS("data_working/QcQ2_unfiltered_181sites_051023.rds") %>%
  mutate(Qc_Q2yr = Qc/RI_2yr_Q_cms)

# A dataset with long names for IDing purposes.
dat_names <- readRDS("data_working/NWIS_Info_181riv_HUC2_df_050923.rds") %>%
  dplyr::select(site_no, station_nm)

# And all covariate data at all sites.
dat_cov <- readRDS("data_working/covariate_data_181sites_070523.rds")

# And 2 yr flood data at all sites.
dat_2yrQ <- read_csv("data_working/RI_2yr_flood_180riv_050923.csv")

# And 10yr flood values for all sites generated by the script
# "Discharge_TS_10yearflood_207riv.R"
dat_10yr <- read_csv("data_working/RI_10yr_flood_206riv.csv")

# Import data used to fit posthoc models.
dat_a1 <- readRDS("data_posthoc_modelfits/accrual_datin_110723.rds")
dat_q1 <- readRDS("data_posthoc_modelfits/qc_datin_110723.rds")

# As well as posthoc model fits from step 13 script.
a1 <- readRDS("data_posthoc_modelfits/accrual_brms_110723.rds")
q1 <- readRDS("data_posthoc_modelfits/qcq2_brms_110723.rds")

#### Reviewer 1 ####

##### Normalizing instead to Q10 #####

# First, need to combine raw input data (dat_in) with available estimates
# of the 10 yr flood at the 6 sites at which I will be re-running the model.

# These same 6 sites are also used to answer other reviewer comments below.

dat_in6 <- dat_in %>%
  filter(site_name %in% c("nwis_06795500", "nwis_02217643", 
                          "nwis_05082500", "nwis_04176500", 
                          "nwis_06893350", "nwis_07075250"))

dat_in6 <- left_join(dat_in6, dat_10yr, by = c("site_name"))

# Create new column with discharge (Q) relativized to the 10yr flood 
# rather than the maximum Q in a given record.

dat_in6$Q_relQ10 <- (dat_in6$Q/dat_in6$RI_10yr_Q_cms)

# Split into a list by ID
l6 <- split(dat_in6, dat_in6$site_name)

# Export dataset for future use.
saveRDS(l6, "data_working/list_6sites_Q10norm_SavoySL.rds")

# Rename source data
df <- l6

# Stan data prep
rstan_options(auto_write=TRUE)
# Specify number of cores
options(mc.cores=6)

# Compile necessary data
stan_data_compile <- function(x){
  data <- list(Ndays=length(x$GPP), # number of days/records
               light = x$light_rel, # relativized light (to maximum light)
               GPP = x$GPP,         # GPP estimates
               GPP_sd = x$GPP_sd,   # standard deviation of GPP estimates
               tQ = x$Q_relQ10,     # relativized discharge (to Q_10yr)
               new_e = x$new_e)     # instances of re-initialization needed
  
  return(data)
}

stan_data_l <- lapply(df, function(x) stan_data_compile(x))

# Latent Biomass (Ricker population) Model
# sets initial values to help chains converge
init_Ricker <- function(...) {
  list(r = 0.2, lambda = -0.03, c = 0.5, s = 1.5) # values to match priors
}

## export results
PM_outputlist_Ricker <- lapply(stan_data_l,
                               function(x) stan("code/beartooth_spring23/Stan_ProductivityModel.stan",
                                           data = x, 
                                           chains = 3,
                                           iter = 5000,
                                           init = init_Ricker,
                                           control = list(max_treedepth = 12)))

# saveRDS(PM_outputlist_Ricker, "data_pinyon/pinyon_6rivers_output_2023_07_12.rds")

# Extract only c and s data from the model
# Going to create a function to pull out all iterations.
extract_cs <- function(df){
  rstan::extract(df, c("c", "s"))
}

data_out6_cs <- map(PM_outputlist_Ricker, extract_cs)

# Save this out too
# saveRDS(data_out6_cs,
#        file = "data_working/pinyon_6rivers_model_cs_params_all_iterations_071223.rds")

# And now use a new function to calculate median and 95%tiles.
# Obtain summary statistics using new "extract_summary" function.
# Note, this pulls diagnostics for site-level parameters.
extract_summary <- function(x){
  df <- x
  df1 <- summary(df,
                 pars = c("c", "s"),
                 probs = c(0.025, 0.5, 0.975))$summary # 2.5% and 97.5% percentiles
  as.data.frame(df1) %>% rownames_to_column("parameter")
}

# And now map this to the output list. 
data_out6_diags <- map(PM_outputlist_Ricker, extract_summary)

# Save this out too
# saveRDS(data_out6_diags,
#        file = "data_working/pinyon_6rivers_model_cs_params_diags_071323.rds")

# Take list above and make into a df.
data_out6_diags_df <- map_df(data_out6_diags, ~as.data.frame(.x), .id="site_name") %>%
  # and for now, I'm going to focus solely on "c" values
  filter(parameter == "c")

dat_QcQ10_6 <- left_join(data_out6_diags_df, dat_10yr) %>%
  rename(c_med = `50%`)

# Calculate Qc in cms.
dat_QcQ10_6 <- dat_QcQ10_6 %>%
  mutate(Qc = c_med * RI_10yr_Q_cms,
         Qc2.5 = `2.5%` * RI_10yr_Q_cms,
         Qc97.5 = `97.5%` * RI_10yr_Q_cms)

# Filter the larger dataset that includes values from when Q was normalized
# to the maximum Q of the dataset.
dat_QcQmax_6 <- dat_Qc_all %>%
  filter(site_name %in% c("nwis_06795500", "nwis_02217643", 
         "nwis_05082500", "nwis_04176500", 
         "nwis_06893350", "nwis_07075250"))

# Join the two datasets.
dat_QcQ10_6$Normalization <- "Q10"
dat_QcQmax_6$Normalization <- "Qmax"

dat_Qc_both_6 <- full_join(dat_QcQ10_6, dat_QcQmax_6) %>%
  # make norm a factor %>%
  mutate(Normalization = factor(Normalization, levels = c("Qmax", "Q10"))) %>%
  # reorder sites and add names for plotting
  mutate(Site = factor(case_when(site_name == "nwis_02217643" ~ "Parks Creek, GA",
                          site_name == "nwis_06893350" ~ "Tomahawk Creek, KS",
                          site_name == "nwis_07075250" ~ "S. Fork Little Red River, AR",
                          site_name == "nwis_05082500" ~ "Red River, ND",
                          site_name == "nwis_04176500" ~ "River Raisin, MI",
                          site_name == "nwis_06795500" ~ "Shell Creek, NE"),
                       levels = c("S. Fork Little Red River, AR", "Tomahawk Creek, KS", 
                                  "River Raisin, MI", "Red River, ND", 
                                  "Parks Creek, GA", "Shell Creek, NE")))

(fig_c <- ggplot(dat_Qc_both_6, aes(x = Qc, y = Site, color = Normalization)) +
    geom_point(size = 5, alpha = 0.75, position = position_dodge(width = -0.5)) +
    geom_errorbarh(aes(xmin = Qc2.5, 
                       xmax = Qc97.5), 
                   height = 0.25,
                   position = position_dodge(width = -0.5)) +
    labs(y = "Site", x = "Qc (cms)") +
    scale_color_manual(values = c("black", "grey")) +
    theme_bw())

# And export.
# ggsave(fig_c,
#        filename = "figures/beartooth_spring23/c_q10_vs_qmaxnorm_fig.jpg",
#        width = 16,
#        height = 10,
#        units = "cm")

##### Pred GPP in the South Fork Iowa ######

# First, need to create the function for predicting GPP.
PM_Ricker <- function(r, lambda, s, c, sig_p, sig_o, df) {
  
  ## Data
  Ndays <- length(df$GPP)
  GPP <- df$GPP
  GPP_sd <- df$GPP_sd
  light <- df$light_rel
  tQ <- df$Q_rel # discharge standardized to max value
  new_e <- df$new_e
  
  ## Vectors for model output of P, B, pred_GPP
  P <- numeric(Ndays)
  P[1] <- 1
  for(i in 2:length(tQ)){
    P[i] = exp(-exp(s*100*(tQ[i] - c)))
  }
  
  B <- numeric(Ndays)
  B[1] <- log(GPP[1]/light[1])
  pred_GPP <- numeric(Ndays)
  pred_GPP[1] <- light[1]*exp(B[1])
  
  ## Process Model
  for(j in 2:Ndays){
    # adding in section for my re-initialization functionality
    if (new_e[j]==1) {
      
      B[j] ~ MCMCglmm::rtnorm(1, mean = log(GPP[j]/light[j])) }
    
    else {
      
      B[j] <- MCMCglmm::rtnorm(1, mean = (B[j-1] + r + lambda*exp(B[j-1]))*P[j],
                               sd = sig_p, upper = 5) }
    
  }
  
  for(i in 2:Ndays){
    pred_GPP[i] <- MCMCglmm::rtnorm(1, mean = light[i]*exp(B[i]), 
                                    sd = sig_o, lower = 0.01)
  }
  
  return(pred_GPP)
}

# Next, need to write the function with which to perform the simulation.
Ricker_sim_fxn <- function(y, x){
  # identify data
  output <- y # Teton/stan output
  df <- x # original data input
  
  # extracted parameters from STAN output already
  pars <- output
  
  # create empty matrix with days of GPP x length of iterations to receive values
  simmat <- matrix(NA, length(df$GPP), length(unlist(pars$sig_p)))
  rmsemat <- matrix(NA, length(df$GPP), 1)
  
  # simulate pred_GPP holding a parameter set for a given iteration constant
  # and then predict forward for a site's timeseries (i.e., length(df$GPP))
  for(i in 1:length(pars$r)){
    simmat[,i] <- PM_Ricker(pars$r[i], pars$lambda[i], pars$s[i], pars$c[i], pars$sig_p[i], pars$sig_p[i], df)
    rmsemat[i] <- sqrt(sum((simmat[,i] - df$GPP)^2)/length(df$GPP))
  }
  
  l <- list(simmat, rmsemat)
  return(l)
  
}

# Adding the nRMSE calculation into the function above didn't play nicely with
# the list that existed, so calculating outside instead.
nRMSE_fxn <- function(df, df_orig){
  
  # Calculate the mean RMSE value for each site.
  nRMSE <- mean(df)/(max(df_orig$GPP) - min(df_orig$GPP))
  
}

# Take list containing all iterations of parameters and make into a df.
dat_out_df <- map_df(dat_out, ~as.data.frame(.x), .id="site_name")

# Trimming input and output datasets for the site of interest.
dat_out1df <- dat_out_df %>%
  filter(site_name %in% c("nwis_05451210"))

dat_in1df <- dat_in %>%
  filter(site_name %in% c("nwis_05451210"))

dat_in1 <- split(dat_in1df, dat_in1df$site_name)

# Re-simulating using all output iterations. Started ~1:10, Ended ~??
Ricker_sim1 <- Ricker_sim_fxn(dat_out1df, dat_in1df)

# Exporting to save progress.
# saveRDS(Ricker_sim1, "data_working/Ricker_sim_1site_081623.rds")

# And for each day, I would like to calculate 2.5%, 50%, and 97.5%tiles.

# Going to pull out just the predicted GPP values.
# So, making a list of odd numbers to pull out predGPP values.
data_1site_gpp <- Ricker_sim1[1]

# Calculate median and confidence intervals
quantile25 <- function(x){quantile(x, probs = 0.025, na.rm = TRUE)}
quantile975 <- function(x){quantile(x, probs = 0.975, na.rm = TRUE)}

pred_gpp1 <- lapply(data_1site_gpp, 
                    function(x) cbind(apply(x, 1, median),
                                      apply(x, 1, quantile25),
                                      apply(x, 1, quantile975)))

# Pull out original GPP values used and sequence #s (for plotting)
orig_gpp_date1 <- lapply(dat_in1, function(x) x %>% dplyr::select(date, GPP, seq,
                                                                  Q, Q_rel))

# Add names to confidence interval lists
my_names <- c("nwis_05451210")

names(pred_gpp1) <- my_names

pred_gpp1 <- lapply(pred_gpp1, function(x) as.data.frame(x) %>% 
                      rename("Median" = "V1",
                             "q2.5" = "V2",
                             "q97.5" = "V3")) # OMG YAY!!!!

# Bind into a single dataframe
keys <- unique(c(names(orig_gpp_date1), names(pred_gpp1)))
df_pred1 <- setNames(Map(cbind, orig_gpp_date1[keys], pred_gpp1[keys]), keys)

# And finally, calculate the normalized RMSE.
rmse1 <- Ricker_sim1[2]

nRMSE_1site <- mapply(nRMSE_fxn, rmse1, dat_in1)

# And plot
# South Fork Iowa River, IA
# First panel GPP
(gpp_plot_rmse1a <- ggplot(df_pred1$nwis_05451210 %>%
                             filter(date < ymd(as.character("2008-12-31"))), 
                           aes(date, GPP)) +
    geom_point(size = 2, color = "#303018") +
    geom_line(aes(date, Median), color = "#609048", linewidth = 1.2) +
    labs(y = expression('GPP (g '*~O[2]~ m^-2~d^-1*')'),
        x = "Date") +
    scale_x_date(date_labels = "%b %Y", date_breaks = "3 months") +
    geom_ribbon(aes(ymin = q2.5,
                    ymax = q97.5),
                fill = "#90A860", alpha = 0.3) +
    annotate(geom = "text", x = date("2008-09-01"), y = 17.5,
             label = paste("nRMSE = ", round(nRMSE_1site[1], 
                           digits = 2)), size = 4) +
    theme_bw() +
    theme(legend.position = "none",
          title = element_text(size = 10),
          axis.title.x = element_text(size=10),
          axis.text.x = element_text(size=10),
          axis.text.y = element_text(size=10),
          axis.title.y = element_text(size=10)))

# First panel Q
(gpp_plot_rmse1a_q <- ggplot(df_pred1$nwis_05451210 %>%
                             filter(date < ymd(as.character("2008-12-31"))), 
                           aes(date, Q)) +
    geom_line(color = "#5792CC", linewidth = 1.2) +
    geom_hline(yintercept = 41.6, size = 1) +
    geom_hline(yintercept = 28.8, size = 1, linetype = "dashed") +
    labs(y = expression('Discharge ('*m^3~s^-1*')'),
         x = "Date") +
    scale_x_date(date_labels = "%b %Y", date_breaks = "3 months") +
    ylim(0, 170) +
    theme_bw() +
    theme(legend.position = "none",
          title = element_text(size = 10),
          axis.title.x = element_text(size=10),
          axis.text.x = element_text(size=10),
          axis.text.y = element_text(size=10),
          axis.title.y = element_text(size=10)))

# Second panel
(gpp_plot_rmse1b <- ggplot(df_pred1$nwis_05451210 %>%
                             filter(date > ymd(as.character("2009-12-31"))), 
                           aes(date, GPP)) +
    geom_point(size = 2, color = "#303018") +
    geom_line(aes(date, Median), color = "#609048", linewidth = 1.2) +
    labs(y = expression('GPP (g '*~O[2]~ m^-2~d^-1*')'),
         x = "Date") +
    scale_x_date(date_labels = "%b %Y", date_breaks = "3 months") +
    geom_ribbon(aes(ymin = q2.5,
                    ymax = q97.5),
                fill = "#90A860", alpha = 0.3) +
    theme_bw() +
    theme(legend.position = "none",
          title = element_text(size = 10),
          axis.title.x = element_text(size=10),
          axis.text.x = element_text(size=10),
          axis.text.y = element_text(size=10),
          axis.title.y = element_text(size=10)))

# Second panel Q
(gpp_plot_rmse1b_q <- ggplot(df_pred1$nwis_05451210 %>%
                               filter(date > ymd(as.character("2009-12-31"))), 
                             aes(date, Q)) +
    geom_line(color = "#5792CC", linewidth = 1.2) +
    geom_hline(yintercept = 41.6, size = 1) +
    geom_hline(yintercept = 28.8, size = 1, linetype = "dashed") +
    labs(y = expression('Discharge ('*m^3~s^-1*')'),
         x = "Date") +
    scale_x_date(date_labels = "%b %Y", date_breaks = "3 months") +
    ylim(0, 170) +
    theme_bw() +
    theme(legend.position = "none",
          title = element_text(size = 10),
          axis.title.x = element_text(size=10),
          axis.text.x = element_text(size=10),
          axis.text.y = element_text(size=10),
          axis.title.y = element_text(size=10)))

# Now, let's combine the above using patchwork.
(fig_SFIR <- (gpp_plot_rmse1a + gpp_plot_rmse1b +
                gpp_plot_rmse1a_q + gpp_plot_rmse1b_q) +
    plot_layout(nrow = 2) +
    plot_annotation(tag_levels = 'A'))

# And export.
# ggsave(fig_SFIR,
#        filename = "figures/beartooth_spring23/predGPP_SForkIowa_081723.jpg",
#        width = 30,
#        height = 15,
#        units = "cm")

##### New Site Selection for SI Figure #####

# Need to identify 16 sites that span time series length, land cover type,
# light, and flow regimes to address reviewer's comments re: heterogeneity
# of results across site types.

# Create site groupings for land cover.
dev <- c("urban", "agricultural")
undev <- c("grassland", "forested", "water", "wetland")

# Plot using most conservative dataset.
dat_Qc <- dat_Qc %>%
  
  mutate(my_groups = factor(case_when(total_days > 870 & LU_category %in% dev &
                                        cvQ <= 1 & meanLight > 24 ~ "Long_Dev_Steady_Light",
                                      total_days > 870 & LU_category %in% dev &
                                        cvQ <= 1 & meanLight <= 24 ~ "Long_Dev_Steady_Dark",
                                      total_days > 870 & LU_category %in% dev &
                                        cvQ > 1 & meanLight > 24 ~ "Long_Dev_Turb_Light",
                                      total_days > 870 & LU_category %in% dev &
                                        cvQ > 1 & meanLight <= 24 ~ "Long_Dev_Turb_Dark",
                                      total_days > 870 & LU_category %in% undev &
                                        cvQ <= 1 & meanLight > 24 ~ "Long_Undev_Steady_Light",
                                      total_days > 870 & LU_category %in% undev &
                                        cvQ <= 1 & meanLight <= 24 ~ "Long_Undev_Steady_Dark",
                                      total_days > 870 & LU_category %in% undev &
                                        cvQ > 1 & meanLight > 24 ~ "Long_Undev_Turb_Light",
                                      total_days > 870 & LU_category %in% undev &
                                        cvQ > 1 & meanLight <= 24 ~ "Long_Undev_Turb_Dark",
                                      total_days <= 870 & LU_category %in% dev &
                                        cvQ <= 1 & meanLight > 24 ~ "Short_Dev_Steady_Light",
                                      total_days <= 870 & LU_category %in% dev &
                                        cvQ <= 1 & meanLight <= 24 ~ "Short_Dev_Steady_Dark",
                                      total_days <= 870 & LU_category %in% dev &
                                        cvQ > 1 & meanLight > 24 ~ "Short_Dev_Turb_Light",
                                      total_days <= 870 & LU_category %in% dev &
                                        cvQ > 1 & meanLight <= 24 ~ "Short_Dev_Turb_Dark",
                                      total_days <= 870 & LU_category %in% undev &
                                        cvQ <= 1 & meanLight > 24 ~ "Short_Undev_Steady_Light",
                                      total_days <= 870 & LU_category %in% undev &
                                        cvQ <= 1 & meanLight <= 24 ~ "Short_Undev_Steady_Dark",
                                      total_days <= 870 & LU_category %in% undev &
                                        cvQ > 1 & meanLight > 24 ~ "Short_Undev_Turb_Light",
                                      total_days <= 870 & LU_category %in% undev &
                                        cvQ > 1 & meanLight <= 24 ~ "Short_Undev_Turb_Dark",
                                      TRUE ~ NA),
                            levels = c("Long_Dev_Steady_Light",
                                       "Long_Dev_Steady_Dark",
                                       "Long_Dev_Turb_Light",
                                       "Long_Dev_Turb_Dark",
                                       "Long_Undev_Steady_Light",
                                       "Long_Undev_Steady_Dark",
                                       "Long_Undev_Turb_Light",
                                       "Long_Undev_Turb_Dark",
                                       "Short_Dev_Steady_Light",
                                       "Short_Dev_Steady_Dark",
                                       "Short_Dev_Turb_Light",
                                       "Short_Dev_Turb_Dark",
                                       "Short_Undev_Steady_Light",
                                       "Short_Undev_Steady_Dark",
                                       "Short_Undev_Turb_Light",
                                       "Short_Undev_Turb_Dark")))

dat_r_Qc_plus <- left_join(dat_Qc, dat_amax %>% dplyr::select(site_name, yield_med))

(viz_fig <- ggplot(dat_r_Qc_plus, aes(x = yield_med, y = Qc_Q2yr)) +
    geom_jitter(alpha = 0.8, size = 3, 
                aes(color = my_groups, text = site_name)) +
    scale_y_log10() +
    scale_x_log10() +
    labs(x = "Maximum Daily Accrual",
         y = "Qc:Q2yr",
         color = "Groupings") +
    theme_bw())

# ggsave(viz_fig,
#        filename = "figures/beartooth_spring23/Sites_16Groups_071123.jpg",
#        width = 15,
#        height = 10,
#        units = "cm") # n = 138

(viz_plotly <- ggplotly(viz_fig))

# Sites I will use for the new figure are: nwis_13213000, nwis_05435950,
# nwis_03219500, nwis_01648010, nwis_04137500, nwis_14211010, nwis_07109500,
# nwis_11044000, nwis_05057200, nwis_12102075, nwis_05451210, nwis_06893970,
# nwis_08447300, nwis_02217643, nwis_04059500, nwis_03538830

# Note, the revised version of the Supplementary figure displaying predicted 
# GPP at multiple sites can be found in the "14_Appendix_figures.R" script.

# Additional note, these sites are all longer than 180 days so they do not
# need to be revised given the new time series length filter imposed Nov 2023.

##### Effects of Variation in TS Length #####

###### Posthoc model residuals ######

# First, I will examine how the post-hoc model residuals compare to the
# length of each of the time series.

# Create a dataset with only site-names and lengths of ts
ts_lengths <- dat_in %>%
  group_by(site_name) %>%
  summarize(days = n()) %>% # count the number of rows/days
  ungroup()

# Extract residuals for first amax posthoc model per code found here:
# http://paul-buerkner.github.io/brms/reference/residuals.brmsfit.html
ra1 <- residuals(a1,
                 method = "posterior_predict",
                 summary = TRUE,
                 probs = c(0.025, 0.975))

# Join with ts length.
dat_a1_137 <- left_join(dat_a1, ts_lengths)

# And join with residuals
dat_a1_137 <- cbind(dat_a1_137, ra1)

# Plot residuals of accrual posthoc model vs. time series length
(fig_resids_a <- ggplot(dat_a1_137, aes(x = days, y = Estimate)) +
  geom_point(size = 3, alpha = 0.75) +
  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.75) +
  labs(x = "Time Series Length (days)",
       y = "Accrual Model Residual Estimate") +
  theme_bw())

# Extract residuals for second QcQ2yr posthoc model using same protocol
# as described above.
rq1 <- residuals(q1,
                 method = "posterior_predict",
                 summary = TRUE,
                 probs = c(0.025, 0.975))

# Join with ts length.
dat_q1_124 <- left_join(dat_q1, ts_lengths)

# And join with residuals
dat_q1_124 <- cbind(dat_q1_124, rq1)

# Plot residuals of accrual posthoc model vs. time series length
(fig_resids_q <- ggplot(dat_q1_124, aes(x = days, y = Estimate)) +
    geom_point(size = 3, alpha = 0.75) +
    geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.75) +
    labs(x = "Time Series Length (days)",
         y = "Critical Disturbance Threshold\nModel Residual Estimate") +
    theme_bw())

# Join figures together.
(fig_resids <- fig_resids_a + fig_resids_q +
  plot_annotation(tag_levels = 'A'))

# And export.
# ggsave(fig_resids,
#        filename = "figures/beartooth_spring23/resids_ts_length_110823.jpg",
#        width = 20,
#        height = 10,
#        units = "cm")

###### Model fit with days as covariate ######

# And as an added gut check, I will re-run the model fits to examine
# time series length as covariate.

# Join raw dataset with time series lengths.
dat_amax_ts <- left_join(dat_amax, ts_lengths)

dat_amax_ts <- dat_amax_ts %>%
  mutate(log_yield = log10(yield_med)) %>% # log-transform yield
  mutate(log_width = log10(width_med)) %>% # log-transform width
  mutate(log_days = log10(days)) %>% # log-transform length
  # Creating a new categorical dam column to model by.
  mutate(Dam_binary = factor(case_when(
    Dam %in% c("50", "80", "95") ~ "0", # Potential = 5-50%
    Dam == "0" ~ "1", # Certain = 100%
    TRUE ~ NA)))

# Necessary variables have already been log-transformed and
# now just need to be scaled.
dat_amax_ts_brms1 <- dat_amax_ts %>%
  # assigning sites to be rownames so that we can re-identify and add HUC2
  # and Dams back in once we've scaled the remaining variables
  column_to_rownames(var = "site_name") %>%
  # and removing sites with NAs
  drop_na(log_yield, meanTemp, NHD_RdDensWs,
          log_width, exc_y, log_days,
          Dam_binary, huc_2) %>%
  dplyr::select(log_yield, meanTemp, NHD_RdDensWs, 
                log_width, exc_y, log_days)

dat_amax_ts_brms1_scaled <- scale(dat_amax_ts_brms1) # scale variables

# Pull sites back in so that we can match with HUC2 values.
dat_amax_ts_brms <- rownames_to_column(as.data.frame(dat_amax_ts_brms1_scaled), 
                                    var = "site_name")

dat_amax_ts_Dam_HUC <- dat_amax_ts %>%
  dplyr::select(site_name, Dam_binary, huc_2)

dat_amax_ts_brms <- left_join(dat_amax_ts_brms, dat_amax_ts_Dam_HUC) %>%
  dplyr::select(log_yield, meanTemp, NHD_RdDensWs, 
                Dam_binary, log_width, exc_y, huc_2,
                log_days, site_name) %>%
  mutate(huc_2 = factor(huc_2))

# Model fit for accrual.
a1_ts <- brm(log_yield ~ log_width + NHD_RdDensWs +
               Dam_binary + meanTemp + exc_y + 
               log_days + (1|huc_2), 
          data = dat_amax_ts_brms, family = gaussian())

# Examine model outputs.
summary(a1_ts)

#              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept        0.22      0.16    -0.09     0.54 1.00     2748     2532
# log_width        0.44      0.11     0.23     0.64 1.00     3630     3321
# NHD_RdDensWs     0.00      0.10    -0.20     0.20 1.00     3110     3044
# Dam_binary1     -0.38      0.16    -0.69    -0.06 1.00     5383     2703
# meanTemp         0.04      0.10    -0.16     0.24 1.00     3197     3070
# exc_y            0.19      0.08     0.03     0.34 1.00     5577     2817
# log_days         0.11      0.08    -0.05     0.28 1.00     3673     2831

# No. of days is not significant.

mcmc_plot(a1_ts)

# And now for other model fit (Qc:Q2yr).
# Join raw dataset with time series lengths.
dat_Qc_ts <- left_join(dat_Qc, ts_lengths)

# Going to log transform QcQ2yr too.
dat_Qc_ts <- dat_Qc_ts %>%
  mutate(logQcQ2 = log10(Qc_Q2yr),
         log_width = log10(width_med),
         log_days = log10(days)) %>%
  # Creating the new categorical dam column to model by.
  mutate(Dam_binary = factor(case_when(
    Dam %in% c("50", "80", "95") ~ "0", # Potential effect of dams
    Dam == "0" ~ "1", # Certain effect of dams
    TRUE ~ NA)))

# Necessary variables have already been log-transformed and
# now just need to be scaled.
dat_Qc_ts_brms1 <- dat_Qc_ts %>%
  # assigning sites to be rownames so that we can re-identify and add HUC2
  # back in once we've scaled the remaining variables
  column_to_rownames(var = "site_name") %>%
  # and removing records with NAs 
  drop_na(logQcQ2, NHD_RdDensWs, log_width, log_days,
          Dam_binary, huc_2) %>%
  dplyr::select(logQcQ2, NHD_RdDensWs, log_width, log_days)

dat_Qc_ts_brms1_scaled <- scale(dat_Qc_ts_brms1)

# Pull sites back in so that we can match with HUC2 values.
dat_Qc_ts_brms <- rownames_to_column(as.data.frame(dat_Qc_ts_brms1_scaled), 
                                  var = "site_name")

dat_Qc_ts_Dam_HUC <- dat_Qc_ts %>%
  dplyr::select(site_name, Dam_binary, huc_2)

dat_Qc_ts_brms <- left_join(dat_Qc_ts_brms, dat_Qc_ts_Dam_HUC) %>%
  dplyr::select(logQcQ2, NHD_RdDensWs, 
                Dam_binary, log_width, huc_2,
                log_days, site_name) %>%
  mutate(huc_2 = factor(huc_2))

# Model fit for critical disturbance threshold normalized to 2yr flood.
q1_ts <- brm(logQcQ2 ~ NHD_RdDensWs + Dam_binary + 
               log_width + log_days + (1|huc_2), 
          data = dat_Qc_ts_brms, family = gaussian()) 

# Examine model results.
summary(q1_ts)

#              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept        0.13      0.19    -0.22     0.51 1.00     2024     2248
# NHD_RdDensWs    -0.10      0.12    -0.33     0.13 1.00     3096     2762
# Dam_binary1     -0.17      0.19    -0.53     0.20 1.00     4340     3240
# log_width       -0.02      0.12    -0.26     0.22 1.00     3340     2676
# log_days         0.12      0.10    -0.07     0.30 1.00     4333     2993

# As above, no. of days is not significant.

mcmc_plot(q1_ts)

####### 3-6 mos removed from amax #######

# Now, I will also examine posthoc model results with shorter (3-6 mos) time
# series removed.

# First, I need to create a dataset with the sites to be included listed.
ts_lengths_6up <- ts_lengths %>%
  filter(days > 180) # n = 172 sites remaining

# Join raw dataset with time series lengths.
dat_amax_ts6 <- left_join(dat_amax, ts_lengths_6up) %>%
  # drop sites that have less than 180 days
  drop_na(days) %>% # n = 143 sites remaining
  # and drop remaining sites that have missing covariate data
  drop_na(yield_med, meanTemp, NHD_RdDensWs, width_med, exc_y, 
          Dam, huc_2) # n = 137 sites remaining

dat_amax_ts6 <- dat_amax_ts6 %>%
  mutate(log_yield = log10(yield_med)) %>% # log-transform yield
  mutate(log_width = log10(width_med)) %>% # log-transform width
  # Creating a new categorical dam column to model by.
  mutate(Dam_binary = factor(case_when(
    Dam %in% c("50", "80", "95") ~ "0", # Potential = 5-50%
    Dam == "0" ~ "1", # Certain = 100%
    TRUE ~ NA)))

# Necessary variables have already been log-transformed and
# now just need to be scaled.
dat_amax_ts6_brms1 <- dat_amax_ts6 %>%
  # assigning sites to be rownames so that we can re-identify and add HUC2
  # and Dams back in once we've scaled the remaining variables
  column_to_rownames(var = "site_name") %>%
  dplyr::select(log_yield, meanTemp, NHD_RdDensWs, 
                log_width, exc_y)

dat_amax_ts6_brms1_scaled <- scale(dat_amax_ts6_brms1) # scale variables

# Pull sites back in so that we can match with HUC2 values.
dat_amax_ts6_brms <- rownames_to_column(as.data.frame(dat_amax_ts6_brms1_scaled), 
                                       var = "site_name")

dat_amax_ts6_Dam_HUC <- dat_amax_ts6 %>%
  dplyr::select(site_name, Dam_binary, huc_2)

dat_amax_ts6_brms <- full_join(dat_amax_ts6_brms, dat_amax_ts6_Dam_HUC) %>%
  dplyr::select(log_yield, meanTemp, NHD_RdDensWs, 
                Dam_binary, log_width, exc_y, huc_2,
                site_name) %>%
  mutate(huc_2 = factor(huc_2))

# Model fit for accrual.
a1_ts6 <- brm(log_yield ~ log_width + NHD_RdDensWs +
               Dam_binary + meanTemp + exc_y + (1|huc_2), 
             data = dat_amax_ts6_brms, family = gaussian())

# Examine model outputs.
summary(a1_ts6)

#              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept        0.23      0.16    -0.08     0.54 1.00     2262     1924
# log_width        0.47      0.10     0.28     0.67 1.00     4100     3244
# NHD_RdDensWs     0.01      0.10    -0.19     0.21 1.00     3669     2907
# Dam_binary1     -0.40      0.16    -0.72    -0.09 1.00     5482     3184
# meanTemp         0.01      0.09    -0.17     0.20 1.00     3214     3338
# exc_y            0.18      0.08     0.03     0.33 1.00     6058     3159

# All covariates remain as before.

# Make summary plot.
# Pull out the data.
dat_a1_ts6 <- mcmc_intervals_data(a1_ts6,
                                 point_est = "median", # default = "median"
                                 prob = 0.66, # default = 0.5
                                 prob_outer = 0.95) # default = 0.9

# Making custom plot to change color of each interval.
# Using core dataset "post_data" rather than canned function.
(a_fig1_custom <- ggplot(dat_a1_ts6 %>%
                          filter(parameter %in% c("b_log_width", "b_exc_y",
                                                  "b_NHD_RdDensWs",
                                                  "b_meanTemp", 
                                                  "b_Dam_binary1")) %>%
                          mutate(par_f = factor(parameter, 
                                                levels = c("b_log_width",
                                                           "b_exc_y",
                                                           "b_NHD_RdDensWs",
                                                           "b_meanTemp",
                                                           "b_Dam_binary1"))), 
                        aes(x = m, y = par_f, color = par_f)) +
    geom_linerange(aes(xmin = ll, xmax = hh),
                   size = 2, alpha = 0.5) +
    geom_point(size = 3) +
    vline_at(v = 0) +
    scale_x_continuous(breaks = c(-0.5, 0, 0.5)) +
    labs(x = "Posterior Estimates",
         y = "Predictors",
         title = "> 6mos. - amax") +
    scale_y_discrete(labels = c("b_log_width" = "Width",
                                "b_NHD_RdDensWs" = "Roads",
                                "b_Dam_binary1" = "Dam",
                                "b_meanTemp" = "Temperature",
                                "b_exc_y" = "Exceedances")) +
    theme_bw() +
    scale_color_manual(values = c("#4B8FF7", "#F29F05", "#F29F05", 
                                  "#4B8FF7", "#F29F05")) +
    theme(text = element_text(size = 10),
          legend.position = "none"))

####### 3m-1y removed from amax #######

# Now, I will examine more posthoc model results with shorter (3 mos - 1 year) time
# series removed.

# First, I need to create a dataset with the sites to be included listed.
ts_lengths_1up <- ts_lengths %>%
  filter(days > 365) # n = 134 sites remaining

# Join raw dataset with time series lengths.
dat_amax_ts1 <- left_join(dat_amax, ts_lengths_1up) %>%
  # drop sites that have less than 365 days
  drop_na(days) %>% # n = 107 sites remaining
  # and drop remaining sites that have missing covariate data
  drop_na(yield_med, meanTemp, NHD_RdDensWs, width_med, exc_y, 
          Dam, huc_2) # n = 104 sites remaining

dat_amax_ts1 <- dat_amax_ts1 %>%
  mutate(log_yield = log10(yield_med)) %>% # log-transform yield
  mutate(log_width = log10(width_med)) %>% # log-transform width
  # Creating a new categorical dam column to model by.
  mutate(Dam_binary = factor(case_when(
    Dam %in% c("50", "80", "95") ~ "0", # Potential = 5-50%
    Dam == "0" ~ "1", # Certain = 100%
    TRUE ~ NA)))

# Necessary variables have already been log-transformed and
# now just need to be scaled.
dat_amax_ts1_brms1 <- dat_amax_ts1 %>%
  # assigning sites to be rownames so that we can re-identify and add HUC2
  # and Dams back in once we've scaled the remaining variables
  column_to_rownames(var = "site_name") %>%
  dplyr::select(log_yield, meanTemp, NHD_RdDensWs, 
                log_width, exc_y)

dat_amax_ts1_brms1_scaled <- scale(dat_amax_ts1_brms1) # scale variables

# Pull sites back in so that we can match with HUC2 values.
dat_amax_ts1_brms <- rownames_to_column(as.data.frame(dat_amax_ts1_brms1_scaled), 
                                        var = "site_name")

dat_amax_ts1_Dam_HUC <- dat_amax_ts1 %>%
  dplyr::select(site_name, Dam_binary, huc_2)

dat_amax_ts1_brms <- full_join(dat_amax_ts1_brms, dat_amax_ts1_Dam_HUC) %>%
  dplyr::select(log_yield, meanTemp, NHD_RdDensWs, 
                Dam_binary, log_width, exc_y, huc_2,
                site_name) %>%
  mutate(huc_2 = factor(huc_2))

# Model fit for accrual.
a1_ts1 <- brm(log_yield ~ log_width + NHD_RdDensWs +
                Dam_binary + meanTemp + exc_y + (1|huc_2), 
              data = dat_amax_ts1_brms, family = gaussian())

# Examine model outputs.
summary(a1_ts1)

#              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept        0.24      0.18    -0.10     0.60 1.00     2009     2256
# log_width        0.45      0.12     0.22     0.70 1.00     2931     2989
# NHD_RdDensWs     0.04      0.13    -0.21     0.29 1.00     2566     2831
# Dam_binary1     -0.30      0.17    -0.64     0.03 1.00     5066     3001
# meanTemp        -0.00      0.11    -0.22     0.22 1.00     3176     2996
# exc_y            0.23      0.08     0.06     0.40 1.00     4579     2987

# All covariates remain as before.

# Make summary plot.
# Pull out the data.
dat_a1_ts1 <- mcmc_intervals_data(a1_ts1,
                                  point_est = "median", # default = "median"
                                  prob = 0.66, # default = 0.5
                                  prob_outer = 0.95) # default = 0.9

# Making custom plot to change color of each interval.
# Using core dataset "post_data" rather than canned function.
(a_fig2_custom <- ggplot(dat_a1_ts1 %>%
                           filter(parameter %in% c("b_log_width", "b_exc_y",
                                                   "b_NHD_RdDensWs",
                                                   "b_meanTemp", 
                                                   "b_Dam_binary1")) %>%
                           mutate(par_f = factor(parameter, 
                                                 levels = c("b_log_width",
                                                            "b_exc_y",
                                                            "b_NHD_RdDensWs",
                                                            "b_meanTemp",
                                                            "b_Dam_binary1"))), 
                         aes(x = m, y = par_f, color = par_f)) +
    geom_linerange(aes(xmin = ll, xmax = hh),
                   size = 2, alpha = 0.5) +
    geom_point(size = 3) +
    vline_at(v = 0) +
    scale_x_continuous(breaks = c(-0.5, 0, 0.5)) +
    labs(x = "Posterior Estimates",
         y = "Predictors",
         title = "> 1yr. - amax") +
    scale_y_discrete(labels = c("b_log_width" = "Width",
                                "b_NHD_RdDensWs" = "Roads",
                                "b_Dam_binary1" = "Dam",
                                "b_meanTemp" = "Temperature",
                                "b_exc_y" = "Exceedances")) +
    theme_bw() +
    scale_color_manual(values = c("#4B8FF7", "#F29F05", "#F29F05", 
                                  "#4B8FF7", "#F29F05")) +
    theme(text = element_text(size = 10),
          legend.position = "none"))

######### 3-6 mos removed from Qc #########

# And now for other model fit (Qc:Q2yr).
# Join raw dataset with time series lengths for sites with less than 3 mos.
dat_Qc_ts6 <- left_join(dat_Qc, ts_lengths_6up) %>%
  # drop sites that have less than 180 days
  drop_na(days) %>% # n = 130 sites remaining
  # and drop remaining sites that have missing covariate data
  drop_na(Qc_Q2yr, NHD_RdDensWs, width_med, Dam, huc_2) # n = 124 sites remaining

# Going to log transform QcQ2yr too.
dat_Qc_ts6 <- dat_Qc_ts6 %>%
  mutate(logQcQ2 = log10(Qc_Q2yr),
         log_width = log10(width_med)) %>%
  # Creating the new categorical dam column to model by.
  mutate(Dam_binary = factor(case_when(
    Dam %in% c("50", "80", "95") ~ "0", # Potential effect of dams
    Dam == "0" ~ "1", # Certain effect of dams
    TRUE ~ NA)))

# Necessary variables have already been log-transformed and
# now just need to be scaled.
dat_Qc_ts6_brms1 <- dat_Qc_ts6 %>%
  # assigning sites to be rownames so that we can re-identify and add HUC2
  # back in once we've scaled the remaining variables
  column_to_rownames(var = "site_name") %>%
  dplyr::select(logQcQ2, NHD_RdDensWs, log_width)

dat_Qc_ts6_brms1_scaled <- scale(dat_Qc_ts6_brms1)

# Pull sites back in so that we can match with HUC2 values.
dat_Qc_ts6_brms <- rownames_to_column(as.data.frame(dat_Qc_ts6_brms1_scaled), 
                                     var = "site_name")

dat_Qc_ts6_Dam_HUC <- dat_Qc_ts6 %>%
  dplyr::select(site_name, Dam_binary, huc_2)

dat_Qc_ts6_brms <- left_join(dat_Qc_ts6_brms, dat_Qc_ts6_Dam_HUC) %>%
  dplyr::select(logQcQ2, NHD_RdDensWs, 
                Dam_binary, log_width, huc_2,
                site_name) %>%
  mutate(huc_2 = factor(huc_2))

# Model fit for critical disturbance threshold normalized to 2yr flood.
q1_ts6 <- brm(logQcQ2 ~ NHD_RdDensWs + Dam_binary + 
               log_width + (1|huc_2), 
             data = dat_Qc_ts6_brms, family = gaussian()) 

# Examine model results.
summary(q1_ts6)

#              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept        0.15      0.19    -0.20     0.53 1.00     2192     2381
# NHD_RdDensWs    -0.07      0.12    -0.30     0.18 1.00     3232     2908
# Dam_binary1     -0.20      0.19    -0.58     0.18 1.00     5150     3248
# log_width        0.01      0.12    -0.24     0.25 1.00     3174     3113

# As above, all covariates remain as before - not significant.

# Make summary plot.
# Pull out the data.
dat_q1_ts6 <- mcmc_intervals_data(q1_ts6,
                                  point_est = "median", # default = "median"
                                  prob = 0.66, # default = 0.5
                                  prob_outer = 0.95) # default = 0.9

# And plot.
(q_fig1_custom <- ggplot(dat_q1_ts6 %>%
                           filter(parameter %in% c("b_log_width",
                                                   "b_NHD_RdDensWs",
                                                   "b_Dam_binary1")) %>%
                           mutate(par_f = factor(parameter, 
                                                 levels = c("b_log_width",
                                                            "b_NHD_RdDensWs",
                                                            "b_Dam_binary1"))), 
                         aes(x = m, y = par_f, color = par_f)) +
    geom_linerange(aes(xmin = ll, xmax = hh),
                   size = 2, alpha = 0.5) +
    geom_point(size = 3) +
    vline_at(v = 0) +
    scale_x_continuous(breaks = c(-0.4,-0.2, 0, 0.2)) +
    labs(x = "Posterior Estimates",
         y = "Predictors",
         title = "> 6mos. - Qc:Q2yr") +
    scale_y_discrete(labels = c("b_log_width" = "Width",
                                "b_NHD_RdDensWs" = "Roads",
                                "b_Dam_binary1" = "Dam")) +
    theme_bw() +
    scale_color_manual(values = c("#4B8FF7", "#F29F05", "#F29F05")) +
    theme(text = element_text(size = 10),
          legend.position = "none"))

######### 3m-1y removed from Qc #########

# And now for other model fit (Qc:Q2yr).
# Join raw dataset with time series lengths for sites with less than 3 mos.
dat_Qc_ts1 <- left_join(dat_Qc, ts_lengths_1up) %>%
  # drop sites that have less than 365 days
  drop_na(days) %>% # n = 96 sites remaining
  # and drop remaining sites that have missing covariate data
  drop_na(Qc_Q2yr, NHD_RdDensWs, width_med, Dam, huc_2) # n = 92 sites remaining

# Going to log transform QcQ2yr too.
dat_Qc_ts1 <- dat_Qc_ts1 %>%
  mutate(logQcQ2 = log10(Qc_Q2yr),
         log_width = log10(width_med)) %>%
  # Creating the new categorical dam column to model by.
  mutate(Dam_binary = factor(case_when(
    Dam %in% c("50", "80", "95") ~ "0", # Potential effect of dams
    Dam == "0" ~ "1", # Certain effect of dams
    TRUE ~ NA)))

# Necessary variables have already been log-transformed and
# now just need to be scaled.
dat_Qc_ts1_brms1 <- dat_Qc_ts1 %>%
  # assigning sites to be rownames so that we can re-identify and add HUC2
  # back in once we've scaled the remaining variables
  column_to_rownames(var = "site_name") %>%
  dplyr::select(logQcQ2, NHD_RdDensWs, log_width)

dat_Qc_ts1_brms1_scaled <- scale(dat_Qc_ts1_brms1)

# Pull sites back in so that we can match with HUC2 values.
dat_Qc_ts1_brms <- rownames_to_column(as.data.frame(dat_Qc_ts1_brms1_scaled), 
                                      var = "site_name")

dat_Qc_ts1_Dam_HUC <- dat_Qc_ts1 %>%
  dplyr::select(site_name, Dam_binary, huc_2)

dat_Qc_ts1_brms <- left_join(dat_Qc_ts1_brms, dat_Qc_ts1_Dam_HUC) %>%
  dplyr::select(logQcQ2, NHD_RdDensWs, 
                Dam_binary, log_width, huc_2,
                site_name) %>%
  mutate(huc_2 = factor(huc_2))

# Model fit for critical disturbance threshold normalized to 2yr flood.
q1_ts1 <- brm(logQcQ2 ~ NHD_RdDensWs + Dam_binary + 
                log_width + (1|huc_2), 
              data = dat_Qc_ts1_brms, family = gaussian()) 

# Examine model results.
summary(q1_ts1)

#              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept        0.21      0.23    -0.22     0.68 1.00     2033     2140
# NHD_RdDensWs    -0.16      0.15    -0.46     0.14 1.00     2757     2798
# Dam_binary1     -0.28      0.20    -0.66     0.12 1.00     4733     2959
# log_width       -0.10      0.14    -0.38     0.19 1.00     2902     3042

# As above, all covariates remain as before - not significant.

# Make summary plot.
# Pull out the data.
dat_q1_ts1 <- mcmc_intervals_data(q1_ts1,
                                  point_est = "median", # default = "median"
                                  prob = 0.66, # default = 0.5
                                  prob_outer = 0.95) # default = 0.9

# And plot.
(q_fig2_custom <- ggplot(dat_q1_ts1 %>%
                           filter(parameter %in% c("b_log_width",
                                                   "b_NHD_RdDensWs",
                                                   "b_Dam_binary1")) %>%
                           mutate(par_f = factor(parameter, 
                                                 levels = c("b_log_width",
                                                            "b_NHD_RdDensWs",
                                                            "b_Dam_binary1"))), 
                         aes(x = m, y = par_f, color = par_f)) +
    geom_linerange(aes(xmin = ll, xmax = hh),
                   size = 2, alpha = 0.5) +
    geom_point(size = 3) +
    vline_at(v = 0) +
    scale_x_continuous(breaks = c(-0.4,-0.2, 0, 0.2)) +
    labs(x = "Posterior Estimates",
         y = "Predictors",
         title = "> 1yr. - QC:Q2yr") +
    scale_y_discrete(labels = c("b_log_width" = "Width",
                                "b_NHD_RdDensWs" = "Roads",
                                "b_Dam_binary1" = "Dam")) +
    theme_bw() +
    scale_color_manual(values = c("#4B8FF7", "#F29F05", "#F29F05")) +
    theme(text = element_text(size = 10),
          legend.position = "none"))

# Join all posthoc tests together.
fig_all <- a_fig1_custom + a_fig2_custom +
  q_fig1_custom + q_fig2_custom +
  plot_layout(nrow = 2)

# And export.
# ggsave(fig_all,
#        filename = "figures/beartooth_spring23/brms_ts_tests_101123.tiff",
#        width = 12,
#        height = 10,
#        units = "cm",
#        dpi = 300)

###### Resampling approach ######

# First, I will investigate which of the sites have the longest records and
# pass both amax and c filtering steps.
dat_long <- left_join(dat_Qc, ts_lengths)

# And create a list of these site names.
dat_long10 <- dat_long %>%
  filter(days > 2320)

my10sites <- dat_long10$site_name

# Select these ten sites and then further split them into 5 timeframes each:
# 6 months, 9 months, 1 year, 2 years, and 4 years of data
# to compare parameter estimates when full timeseries is used.

dat_in_10 <- dat_in %>%
  filter(site_name %in% my10sites)

# And join with ts_length.
dat_in_10 <- left_join(dat_in_10, ts_lengths)

# Function for re-sampling
resample5_random <- function(dat){
  
  # dat referring to input data from a single site
  ts_length <- dat$days[1]
  
  # select random indexes (to the nearest day) of timeframes to sample
  # with max values being based on the end of the data length
  mo6 <- round(runif(1, min=1, max=ts_length - 180)) 
  mo9 <- round(runif(1, min=1, max=ts_length - 270)) 
  yr1 <- round(runif(1, min=1, max=ts_length - 365)) 
  yr2 <- round(runif(1, min=1, max=ts_length - 730)) 
  yr4 <- round(runif(1, min=1, max=ts_length - 1460)) 

  # select data according to these randomly selected indices
  # and add new columns for relative L and Q
  dat_mo6 <- dat[mo6:(mo6+180),] %>% 
  mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
         Q_rel = Q/max(Q, na.rm = TRUE),
         sample = "6 months")

  dat_mo9 <- dat[mo9:(mo9+270),] %>% 
  mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
         Q_rel = Q/max(Q, na.rm = TRUE),
         sample = "9 months")

  dat_yr1 <- dat[yr1:(yr1+365),] %>% 
  mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
         Q_rel = Q/max(Q, na.rm = TRUE),
         sample = "1 year")

  dat_yr2 <- dat[yr2:(yr2+730),] %>% 
  mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
         Q_rel = Q/max(Q, na.rm = TRUE),
         sample = "2 years")

  dat_yr4 <- dat[yr4:(yr4+1460),] %>% 
  mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
         Q_rel = Q/max(Q, na.rm = TRUE),
         sample = "4 years")

  # join into a single dataframe
  dat_resample <- rbind(dat_mo6, dat_mo9)
  dat_resample <- rbind(dat_resample, dat_yr1)
  dat_resample <- rbind(dat_resample, dat_yr2)
  dat_resample <- rbind(dat_resample, dat_yr4)
  
  return(dat_resample)

}

# Creating a second function that samples at non-random intervals.
# Function for re-sampling
resample5_build <- function(dat){
  
  # select data according to these randomly selected indices
  # and add new columns for relative L and Q
  # overwrites old columns of relative light and discharge
  dat_mo6 <- dat[1:180,] %>% 
    mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
           Q_rel = Q/max(Q, na.rm = TRUE),
           sample = "6 months")
  
  dat_mo9 <- dat[1:270,] %>% 
    mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
           Q_rel = Q/max(Q, na.rm = TRUE),
           sample = "9 months")
  
  dat_yr1 <- dat[1:365,] %>% 
    mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
           Q_rel = Q/max(Q, na.rm = TRUE),
           sample = "1 year")
  
  dat_yr2 <- dat[1:730,] %>% 
    mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
           Q_rel = Q/max(Q, na.rm = TRUE),
           sample = "2 years")
  
  dat_yr4 <- dat[1:1460,] %>% 
    mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
           Q_rel = Q/max(Q, na.rm = TRUE),
           sample = "4 years")
  
  # join into a single dataframe
  dat_resample <- rbind(dat_mo6, dat_mo9)
  dat_resample <- rbind(dat_resample, dat_yr1)
  dat_resample <- rbind(dat_resample, dat_yr2)
  dat_resample <- rbind(dat_resample, dat_yr4)
  
  return(dat_resample)
  
}

# Test to be sure this function works appropriately at one site.
test <- resample5_build(dat_in_10 %>%
                    filter(site_name == "nwis_01481000"))

ggplot(test, aes(x = date, y = GPP)) +
  geom_point() +
  theme_bw() +
  facet_wrap(.~sample, nrow = 5)

# Split dat_in into a list by ID
l10 <- split(dat_in_10, dat_in_10$site_name)

# Apply function to list of all 10 sites
l10_resample <- lapply(l10, function(dat) resample5_build(dat))

# Test plot at one more site to be sure
ggplot(l10_resample$nwis_08211200, aes(x = date, y = GPP)) +
  geom_point() +
  theme_bw() +
  facet_wrap(.~sample, nrow = 5) # YIPEE :)

# Export dataset for future use.
# saveRDS(l10_resample, "data_working/list_10sites_5ts_Qmaxnorm_SavoySL_102523.rds")

# Join full list into dataframe.
dat10_resample <- do.call(rbind.data.frame, l10_resample)

# Create new column for new list ids.
dat10_resample$site_sample <- paste(dat10_resample$site_name, 
                                    dat10_resample$sample, 
                                    sep = "_")

# Split dat_in into a list by ID
l10_5ts_resample <- split(dat10_resample, dat10_resample$site_sample)

# Rename source data
df <- l10_5ts_resample

# Stan data prep
rstan_options(auto_write=TRUE)
# Specify number of cores
options(mc.cores=6)

# Compile necessary data
stan_data_compile <- function(x){
  data <- list(Ndays=length(x$GPP), # number of days/records
               light = x$light_rel, # relativized light (to maximum light)
               GPP = x$GPP,         # GPP estimates
               GPP_sd = x$GPP_sd,   # standard deviation of GPP estimates
               tQ = x$Q_rel,        # relativized discharge (to maximum Q)
               new_e = x$new_e)     # instances of re-initialization needed
  
  return(data)
}

stan_data_l <- lapply(df, function(x) stan_data_compile(x))

# Latent Biomass (Ricker population) Model
# sets initial values to help chains converge
init_Ricker <- function(...) {
  list(r = 0.2, lambda = -0.03, c = 0.5, s = 1.5) # values to match priors
}

## export results - started at 10:55am
PM_outputlist_Ricker <- lapply(stan_data_l,
                               function(x) stan("code/beartooth_spring23/Stan_ProductivityModel.stan",
                                                data = x, 
                                                chains = 3,
                                                iter = 5000,
                                                init = init_Ricker,
                                                control = list(max_treedepth = 12)))

# saveRDS(PM_outputlist_Ricker, "data_pinyon/pinyon_10rivers_5ts_output_2023_10_25.rds")

# Going to create a function to pull out all iterations.
extract_params <- function(df){
  rstan::extract(df, c("r", "lambda", "c", "s"))
}

data10_out_resample <- map(PM_outputlist_Ricker, extract_params)

# Save this out too
# saveRDS(data10_out_resample,
#        file = "data_working/pinyon_10rivers_5ts_params_all_iterations_102623.rds")

# And now use a new function to calculate median and 95%tiles.
# Obtain summary statistics using new "extract_summary" function.
# Note, this pulls diagnostics for site-level parameters.
extract_summary <- function(x){
  df <- x
  df1 <- summary(df,
                 pars = c("r", "lambda", "c", "s"),
                 probs = c(0.025, 0.5, 0.975))$summary
  as.data.frame(df1) %>% rownames_to_column("parameter")
}

# And now map this to the output list. Be patient - this takes a minute.
data10_out_resample_diags <- map(PM_outputlist_Ricker, extract_summary)

# Save this out too
# saveRDS(data10_out_resample_diags,
#         file = "data_working/pinyon_10rivers_5ts_params_diags_102623.rds")

# Take list above and make into a df.
data_out_resample_c <- map_df(data10_out_resample_diags, 
                                     ~as.data.frame(.x), .id="site_sample") %>%
  # and for now, I'm going to focus solely on "r" and "c" values
  filter(parameter %in% c("c")) %>%
  # and rename to match the other dataset
  rename(c_med = `50%`) %>%
  # and split site_sample column up for joining
  separate_wider_delim(site_sample, "_", names = c("nwis", "siteno", "sample")) %>%
  # and make the site column again
  mutate(site_name = paste(nwis, siteno, sep = "_")) %>%
  dplyr::select(-c(nwis, siteno))

# Filter the larger dataset that includes values from the same site.
dat_out_orig_c <- dat_Qc_all %>%
  filter(site_name %in% my10sites)

# Join the two datasets.
dat_out_orig_c <- dat_out_orig_c %>%
  mutate(sample = case_when(site_name == "nwis_01481000" ~ "5.1 years",
                            site_name == "nwis_01608500" ~ "6.3 years",
                            site_name == "nwis_02168504" ~ "8.7 years",
                            site_name == "nwis_04137500" ~ "7.5 years",
                            site_name == "nwis_05435943" ~ "6.4 years",
                            site_name == "nwis_07109500" ~ "6.5 years",
                            site_name == "nwis_08070200" ~ "6.6 years",
                            site_name == "nwis_08211200" ~ "7.4 years",
                            site_name == "nwis_10133800" ~ "7.4 years",
                            site_name == "nwis_14206950" ~ "7.5 years"))

dat_c_both <- full_join(dat_out_orig_c, data_out_resample_c) %>%
  # make length a factor %>%
  mutate(sample = factor(sample, levels = c("6 months", "9 months",
                                            "1 year", "2 years", "4 years",
                                            "5.1 years", "6.3 years", "6.4 years",
                                            "6.5 years", "6.6 years", "7.4 years",
                                            "7.5 years", "8.7 years"))) %>%
  # add names for plotting
  mutate(run = factor(case_when(sample %in% c("5.1 years", "6.3 years", "6.4 years",
                                              "6.5 years", "6.6 years", "7.4 years",
                                              "7.5 years", "8.7 years") ~ "Original",
                                TRUE ~ "Resample"),
                       levels = c("Original", "Resample")))

(fig_c <- ggplot(dat_c_both, aes(x = c_med, y = sample, color = run)) +
    geom_point(size = 5, alpha = 0.75, position = position_dodge(width = -0.5)) +
    geom_errorbarh(aes(xmin = `2.5%`, 
                       xmax = `97.5%`), 
                   height = 0.25,
                   position = position_dodge(width = -0.5)) +
    labs(y = "Length of Time Series", x = "c", color = "Run") +
    scale_color_manual(values = c("black", "grey")) +
    theme_bw() +
    theme(legend.position = "none") +
    facet_grid(site_name~., scales = "free"))

# Take list above and make into a df.
data_out_resample_r <- map_df(data10_out_resample_diags, 
                              ~as.data.frame(.x), .id="site_sample") %>%
  filter(parameter %in% c("r")) %>%
  rename(r_med = `50%`) %>%
  # and split site_sample column up for joining
  separate_wider_delim(site_sample, "_", names = c("nwis", "siteno", "sample")) %>%
  # and make the site column again
  mutate(site_name = paste(nwis, siteno, sep = "_")) %>%
  dplyr::select(-c(nwis, siteno))

# Filter the larger dataset that includes values from the same site.
dat_out_orig_r <- dat_amax %>%
  filter(site_name %in% my10sites)

# Join the two datasets.
dat_out_orig_r <- dat_out_orig_r %>%
  mutate(sample = case_when(site_name == "nwis_01481000" ~ "5.1 years",
                            site_name == "nwis_01608500" ~ "6.3 years",
                            site_name == "nwis_02168504" ~ "8.7 years",
                            site_name == "nwis_04137500" ~ "7.5 years",
                            site_name == "nwis_05435943" ~ "6.4 years",
                            site_name == "nwis_07109500" ~ "6.5 years",
                            site_name == "nwis_08070200" ~ "6.6 years",
                            site_name == "nwis_08211200" ~ "7.4 years",
                            site_name == "nwis_10133800" ~ "7.4 years",
                            site_name == "nwis_14206950" ~ "7.5 years"))

dat_10names <- dat_names %>%
  filter(site_no %in% unique(dat_out_orig_r$site_no))

dat_r_both <- full_join(dat_out_orig_r, data_out_resample_r) %>%
  # make norm a factor %>%
  mutate(sample = factor(sample, levels = c("6 months", "9 months",
                                            "1 year", "2 years", "4 years",
                                            "5.1 years", "6.3 years", "6.4 years",
                                            "6.5 years", "6.6 years", "7.4 years",
                                            "7.5 years", "8.7 years"))) %>%
  # add names for plotting
  mutate(run = factor(case_when(sample %in% c("5.1 years", "6.3 years", "6.4 years",
                                "6.5 years", "6.6 years", "7.4 years",
                                "7.5 years", "8.7 years") ~ "Original",
                                TRUE ~ "Resample"),
                      levels = c("Original", "Resample"))) %>%
  mutate(long_name = factor(case_when(site_name == "nwis_01481000" ~ "Brandywine Creek, PA",
                               site_name == "nwis_01608500" ~ "S. Branch Potomac River, WV",
                               site_name == "nwis_02168504" ~ "Saluda River, SC",
                               site_name == "nwis_04137500" ~ "Au Sable River, MI",
                               site_name == "nwis_05435943" ~ "Badger Mill Creek, WI",
                               site_name == "nwis_07109500" ~ "Arkansas River, CO",
                               site_name == "nwis_08070200" ~ "E. Fork San Jacinto River, TX",
                               site_name == "nwis_08211200" ~ "Nueces River, TX",
                               site_name == "nwis_10133800" ~ "E. Canyon Creek, UT",
                               site_name == "nwis_14206950" ~ "Fanno Creek, OR"),
                            levels = c("Brandywine Creek, PA",
                                       "S. Branch Potomac River, WV",
                                       "Saluda River, SC",
                                       "Au Sable River, MI",
                                       "Badger Mill Creek, WI",
                                       "Arkansas River, CO",
                                       "E. Fork San Jacinto River, TX",
                                       "Nueces River, TX",
                                       "E. Canyon Creek, UT",
                                       "Fanno Creek, OR")))

(fig_r <- ggplot(dat_r_both, aes(x = r_med, y = sample, color = run)) +
    geom_point(size = 5, alpha = 0.75, position = position_dodge(width = -0.5)) +
    geom_errorbarh(aes(xmin = `2.5%`, 
                       xmax = `97.5%`), 
                   height = 0.25,
                   position = position_dodge(width = -0.5)) +
    labs(y = "Length of Time Series", x = "r", color = "Run") +
    scale_color_manual(values = c("black", "gray")) +
    theme_bw() +
    theme(legend.position = "none") +
    facet_wrap(vars(long_name), nrow = 2, scales = "free_y"))

# And export.
# ggsave(fig_r,
#        filename = "figures/beartooth_spring23/r_ts_length_10sites_fig_110823.jpg",
#        width = 30,
#        height = 10,
#        units = "cm")

# Combine into a single figure.
(fig_r_and_c <- fig_r + fig_c)

# And export.
# ggsave(fig_r_and_c,
#        filename = "figures/beartooth_spring23/r_and_c_ts_length_10sites_fig_102623.jpg",
#        width = 20,
#        height = 40,
#        units = "cm")

# Also going to examine flow regimes at these ten sites.
(fig_q <- ggplot(dat_in_10, aes(x = Q)) +
    geom_histogram(bins = 20) +
    labs(y = "Count", x = "Discharge") +
    theme_bw() +
    facet_wrap(site_name~., nrow = 2, scales = "free"))

# And export.
# ggsave(fig_q,
#        filename = "figures/beartooth_spring23/q_10sites_hist_102623.jpg",
#        width = 30,
#        height = 10,
#        units = "cm")

(fig_q_ts <- ggplot(dat_in_10, aes(x = Date, y = Q, group = year)) +
    geom_line() +
    labs(x = "Date", y = "Discharge") +
    theme_bw() +
    facet_wrap(site_name~., nrow = 2, scales = "free"))

# And export.
# ggsave(fig_q_ts,
#        filename = "figures/beartooth_spring23/q_10sites_ts_102623.jpg",
#        width = 30,
#        height = 10,
#        units = "cm")

dat_10_skew <- dat_in_10 %>%
  group_by(site_name) %>%
  summarize(skew = skewness(Q, na.rm=TRUE),
            kurt = kurtosis(Q, na.rm=TRUE),
            CV = sd(Q, na.rm = TRUE)/mean(Q, na.rm = TRUE)) %>%
  ungroup()

# Also going to examine GPP at these ten sites.
(fig_gpp_ts <- ggplot(dat_in_10, aes(x = Date, y = GPP, group = year)) +
    geom_line(color = "chartreuse3") +
    labs(x = "Date", y = "Discharge") +
    theme_bw() +
    facet_wrap(site_name~., nrow = 2, scales = "free"))

# And export.
# ggsave(fig_gpp_ts,
#        filename = "figures/beartooth_spring23/gpp_10sites_hist_103023.jpg",
#        width = 30,
#        height = 10,
#        units = "cm")

saluda_gpp <- dat_in_10 %>%
  filter(site_name == "nwis_02168504") %>%
  group_by(year) %>%
  summarize(meanGPP = mean(GPP, na.rm = TRUE),
            sdGPP = sd(GPP, na.rm = TRUE),
            minGPP = min(GPP, na.rm = TRUE),
            maxGPP = max(GPP, na.rm = TRUE),
            cvGPP = sd(GPP, na.rm = TRUE)/mean(GPP, na.rm = TRUE)) %>%
  ungroup()

###### Overlapping TS ######

# Instead of resampling, this code chunk will explore model estimates based
# on maximizing data overlap.

# First need to examine which dates have the densest coverage.

(time_fig <- ggplot(dat_in, aes(Date)) +
  geom_bar())

# So, 2013-2015 appear to have the most consistent coverage.
# Let's filter out for only those years
dat_3yr <- dat_in %>%
  filter(Date > ymd("2012-12-31")) %>%
  filter(Date < ymd("2016-01-01"))

unique(dat_3yr$site_name) # 143 sites remaining

dat_3yr_ts <- dat_3yr %>%
  group_by(site_name, year) %>%
  summarize(days = n()) %>%
  ungroup()

(days_fig <- ggplot(dat_3yr_ts, aes(factor(year), days)) +
    geom_jitter(color = "grey50") +
    geom_boxplot(fill = "aquamarine", alpha = 0.5) +
    theme_bw()) 

# So, coverage is actually quite high.
dat_3yr_tsmed <- dat_3yr_ts %>%
  group_by(year) %>%
  summarize(med = median(days)) %>%
  ungroup()

# Filter for sites with >300 days per year.
dat_3yr_ts_300 <- dat_3yr_ts %>%
  pivot_wider(values_from = days, names_from = year) %>%
  filter(`2013` > 300) %>%
  filter(`2014` > 300) %>%
  filter(`2015` > 300) # 24 sites remaining

sites24 <- dat_3yr_ts_300$site_name

dat_3yr_300d <- dat_3yr %>%
  filter(site_name %in% sites24) %>% 
  # re-calculate relative light and discharge values
  mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
         Q_rel = Q/max(Q, na.rm = TRUE),
         sample = "2013-2015 300+ days")

# Split dat_in into a list by ID
l24_3yr_300d <- split(dat_3yr_300d, dat_3yr_300d$site_name)

# Rename source data
df <- l24_3yr_300d

# Stan data prep
rstan_options(auto_write=TRUE)
# Specify number of cores
options(mc.cores=6)

# Compile necessary data
stan_data_compile <- function(x){
  data <- list(Ndays=length(x$GPP), # number of days/records
               light = x$light_rel, # relativized light (to maximum light)
               GPP = x$GPP,         # GPP estimates
               GPP_sd = x$GPP_sd,   # standard deviation of GPP estimates
               tQ = x$Q_rel,        # relativized discharge (to maximum Q)
               new_e = x$new_e)     # instances of re-initialization needed
  
  return(data)
}

stan_data_l <- lapply(df, function(x) stan_data_compile(x))

# Latent Biomass (Ricker population) Model
# sets initial values to help chains converge
init_Ricker <- function(...) {
  list(r = 0.2, lambda = -0.03, c = 0.5, s = 1.5) # values to match priors
}

## export results - started at 1:47pm
PM_outputlist_Ricker <- lapply(stan_data_l,
                               function(x) stan("code/beartooth_spring23/Stan_ProductivityModel.stan",
                                                data = x, 
                                                chains = 3,
                                                iter = 5000,
                                                init = init_Ricker,
                                                control = list(max_treedepth = 12)))

# saveRDS(PM_outputlist_Ricker,
#         "data_pinyon/pinyon_24rivers_3yr300d_output_2023_10_31.rds")

# Going to create a function to pull out all iterations.
extract_params <- function(df){
  rstan::extract(df, c("r", "lambda", "c", "s"))
}

data24_out <- map(PM_outputlist_Ricker, extract_params)

# Save this out too
# saveRDS(data24_out,
#        file = "data_working/pinyon_24rivers_3yr300d_params_all_iterations_110123.rds")

# And now use a new function to calculate median and 95%tiles.
# Obtain summary statistics using new "extract_summary" function.
# Note, this pulls diagnostics for site-level parameters.
extract_summary <- function(x){
  df <- x
  df1 <- summary(df,
                 pars = c("r", "lambda", "c", "s"),
                 probs = c(0.025, 0.5, 0.975))$summary
  as.data.frame(df1) %>% rownames_to_column("parameter")
}

# And now map this to the output list. Be patient - this takes a minute.
data24_out_diags <- map(PM_outputlist_Ricker, extract_summary)

# Save this out too
# saveRDS(data24_out_diags,
#         file = "data_working/pinyon_24rivers_3yr300d_params_diags_110323.rds")

# Take lists and make into a df.
dat24_out_df <- map_df(data24_out, ~as.data.frame(.x), .id="site_name")
dat24_diag <- map_df(data24_out_diags, ~as.data.frame(.x), .id="site_name")

####### posthoc  models ######

# Annotating this as well as I can since it includes steps across multiple 
# scripts in my typical workflow.

# STEP 1 - R PARAMETER FILTERS

# First need to filter r and c results.
# r must be positive
dat24_out_rmed <- dat24_out_df %>%
  group_by(site_name) %>%
  summarize(r_med = median(r)) %>%
  ungroup() %>%
  filter(r_med > 0) 

# and rhat must be less than 1.05
dat24_diag_rmed_Rhat <- dat24_diag %>%
  filter(parameter == "r") %>%
  filter(Rhat < 1.05)

dat24_out_rmed_Rhat <- inner_join(dat24_diag_rmed_Rhat, dat24_out_rmed) 

# STEP 2 - AMAX CALCULATIONS

# and calculate maximum accrual
# Create new columns to calculate yield for EACH ITERATION at each site.
dat24_out_df <- dat24_out_df %>%
  mutate(yield = (r*(0.5-(0.07*r)))/-lambda)

# Calculate median accrual values for each site.
dat24_out_yield_med <- dat24_out_df %>%
  group_by(site_name) %>%
  summarize(yield_med = median(yield))

# join with filtered r data above
dat24_out_rmed_Rhat_amax <- left_join(dat24_out_rmed_Rhat, dat24_out_yield_med)

# STEP 3 - C CONVERSION TO QC

# Now need to convert c to Qc before filtering, because this value is included 
# in the amax model.
dat24_out_cmed <- dat24_out_df %>%
  group_by(site_name) %>%
  summarize(c_med = median(c)) %>%
  ungroup()

dat24_diag_c <- dat24_diag %>%
  filter(parameter == "c")

dat24_out_cmed_diag <- left_join(dat24_out_cmed, dat24_diag_c)

# Now, convert normalized c values to typical discharge values.
dat24_maxQ <- dat_3yr_300d %>%
  group_by(site_name) %>%
  summarize(maxQ = max(Q, na.rm = TRUE)) %>%
  ungroup()

dat24_out_cmed_maxQ <- left_join(dat24_out_cmed_diag, dat24_maxQ)

# convert c to Qc values.
dat24_out_cmed_maxQ$Qc <- dat24_out_cmed_maxQ$c_med*dat24_out_cmed_maxQ$maxQ

# Join with 2yr flood data.
dat24_out_cmed_2yrQ <- left_join(dat24_out_cmed_maxQ, dat_2yrQ)

# And determine Qc:Q2yr ratio values.
dat24_out_cmed_2yrQ$Qc_Q2yr <- dat24_out_cmed_2yrQ$Qc/dat24_out_cmed_2yrQ$RI_2yr_Q_cms

# STEP 4 - QC EXCEEDANCES

# Trim down to columns of interest for Qc dataset
dat_Qc_trim <- dat24_out_cmed_2yrQ %>%
  dplyr::select(site_name, Qc)

# Need to add Qc values for each site to the input data
dat24_in_Qc <- inner_join(dat_3yr_300d, dat_Qc_trim)

# And make back into a list to apply the function below.
## split list by ID
dat24_in_Qc_l <- split(dat24_in_Qc, dat24_in_Qc$site_name)

# Event delineation function:
# loop over the separate time sequences for a given site
exceedFUN <- function(d){
  
  # calculate the difference from one day to the next - YES/NO
  
  d <- d %>%
    mutate(exceedance = case_when(Q > Qc ~ "YES", TRUE ~ "NO"))
  
  # delineate sequenced time frames based on day to day differences
  
  d <- d %>% # mark all yesses preceded by nos
    mutate(sequence = case_when(exceedance == "YES" & 
                                  lag(exceedance) == "NO" ~ 1,
                                TRUE ~ 0)) %>%
    # and count total days of exceedance
    mutate(total = case_when(exceedance == "YES" ~ 1,
                             TRUE ~ 0))
  
  return(d)
  
}

# apply event delineation function
dat24_in_exc <- lapply(dat24_in_Qc_l, function(x) exceedFUN(x))

# Make the list back into a df
dat24_in_exc_df <- map_df(dat24_in_exc, ~as.data.frame(.x), .id="site_name")

# And summarize total exceedance events and exceedance days as well as
# total days in a record.
dat24_exceed_sum <- dat24_in_exc_df %>%
  group_by(site_name) %>%
  summarize(total_exc_events = sum(sequence),
            total_exc_days = sum(total),
            total_days = n()) %>%
  ungroup() %>%
  # calculate # of exceedance days relative to length of dataset
  mutate(total_exc_rel_d = total_exc_days/total_days,
         exc_y = total_exc_events/3) # how many per year

# STEP 5 - C PARAMETER FILTERS

# Negative c values are not biologically reasonable, so removing them. 
# Begin by using filtered rmax dataset above.
my_20_site_list <- dat24_out_rmed_Rhat_amax$site_name

dat24_out_cmed_pos <- dat24_out_cmed_2yrQ %>%
  filter(site_name %in% my_20_site_list) %>%
  filter(c_med > 0) %>% # Removes 0 sites. Yay!
  filter(Rhat < 1.05) # An additional 2 sites drop off.

# STEP 6 - MEAN ANNUAL TEMPERATURE

# Last covariate that needs re-calculating is mean annual temperature.
dat24_in_temp <- dat_3yr_300d %>%
  group_by(site_name) %>%
  summarize(meanTemp = mean(temp, na.rm = TRUE)) %>%
  ungroup()

# STEP 7 - FIT AMAX POSTHOC MODEL

# First, I need to combine all the necessary data into a single dataset.
dat24_amax <- left_join(dat24_out_rmed_Rhat_amax, dat24_exceed_sum) # Qc
dat24_amax <- left_join(dat24_amax, dat24_in_temp) # temperature

dat_cov_trim <- dat_cov %>%
  dplyr::select(site_name, NHD_RdDensWs, width_med, Dam, huc_2)

dat24_amax <- left_join(dat24_amax, dat_cov_trim) # and everything else

# Next, appropriate transformations
dat24_amax <- dat24_amax %>%
  mutate(log_yield = log10(yield_med)) %>% # log-transform yield
  mutate(log_width = log10(width_med)) %>% # log-transform width
  # Also creating a new categorical dam column to model by.
  # same metric used in QC:Q2yr model below.
  mutate(Dam_binary = factor(case_when(
    Dam %in% c("50", "80", "95") ~ "0", # Potential = 5-50%
    Dam == "0" ~ "1", # Certain = 100%
    TRUE ~ NA))) %>%
  # and drop NAs
  drop_na(log_yield, meanTemp, NHD_RdDensWs, log_width, exc_y,
          Dam_binary, huc_2)

# Ok, and making the final dataset with which to build models
# where necessary variables have already been log-transformed and
# now just need to be scaled.
dat24_amax_brms1 <- dat24_amax %>%
  # assigning sites to be rownames so that we can re-identify and add HUC2
  # and Dams back in once we've scaled the remaining variables
  column_to_rownames(var = "site_name") %>%
  dplyr::select(log_yield, meanTemp, NHD_RdDensWs, log_width, exc_y)

dat24_amax_brms1_scaled <- scale(dat24_amax_brms1) # scale variables

# Pull sites back in so that we can match with HUC2 values.
dat24_amax_brms <- rownames_to_column(as.data.frame(dat24_amax_brms1_scaled), 
                                    var = "site_name")

dat24_amax_Dam_HUC <- dat24_amax %>%
  dplyr::select(site_name, Dam_binary, huc_2)

dat24_amax_brms <- full_join(dat24_amax_brms, dat24_amax_Dam_HUC) %>%
  dplyr::select(log_yield, meanTemp, NHD_RdDensWs, 
                Dam_binary, log_width, exc_y, huc_2,
                site_name) %>%
  mutate(huc_2 = factor(huc_2))

# Fit multi-level model.
a1_24 <- brm(log_yield ~ log_width + NHD_RdDensWs +
            Dam_binary + meanTemp + exc_y + (1|huc_2), 
          data = dat24_amax_brms, family = gaussian()) # 20 sites

# Export model output.
# saveRDS(a1_24, "data_posthoc_modelfits/accrual20_brms_110323.rds")

# Examine model outputs.
summary(a1_24)

#              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept       -0.21      0.46    -1.19     0.69 1.00     1996     1441
# log_width        0.06      0.37    -0.65     0.80 1.00     2687     2736
# NHD_RdDensWs     0.04      0.37    -0.69     0.77 1.00     2714     1825
# Dam_binary1      0.53      0.65    -0.72     1.81 1.00     2999     2632
# meanTemp        -0.27      0.41    -1.09     0.53 1.00     1937     2164
# exc_y           -0.16      0.37    -0.88     0.57 1.00     2408     2267

# Make summary plot.
# Pull out the data.
dat_a1_24 <- mcmc_intervals_data(a1_24,
                                  point_est = "median", # default = "median"
                                  prob = 0.66, # default = 0.5
                                  prob_outer = 0.95) # default = 0.9

# Making custom plot to change color of each interval.
# Using core dataset "post_data" rather than canned function.
(a24_fig_custom <- ggplot(dat_a1_24 %>%
                           filter(parameter %in% c("b_log_width", "b_exc_y",
                                                   "b_NHD_RdDensWs",
                                                   "b_meanTemp", 
                                                   "b_Dam_binary1")) %>%
                           mutate(par_f = factor(parameter, 
                                                 levels = c("b_log_width",
                                                            "b_exc_y",
                                                            "b_NHD_RdDensWs",
                                                            "b_meanTemp",
                                                            "b_Dam_binary1"))), 
                         aes(x = m, y = par_f, color = par_f)) +
    geom_linerange(aes(xmin = ll, xmax = hh),
                   size = 2, alpha = 0.5) +
    geom_point(size = 3) +
    vline_at(v = 0) +
    #scale_x_continuous(breaks = c(-0.5, 0, 0.5)) +
    labs(x = "Posterior Estimates",
         y = "Predictors") +
    scale_y_discrete(labels = c("b_log_width" = "Width",
                                "b_NHD_RdDensWs" = "Roads",
                                "b_Dam_binary1" = "Dam",
                                "b_meanTemp" = "Temperature",
                                "b_exc_y" = "Exceedances")) +
    theme_bw() +
    scale_color_manual(values = c("#4B8FF7", "#F29F05", "#F29F05", 
                                  "#4B8FF7", "#F29F05")) +
    theme(text = element_text(size = 10),
          legend.position = "none"))

# STEP 8 - FIT QC POSTHOC MODEL

# First, I need to combine all the necessary data into a single dataset.
dat_cov_trim2 <- dat_cov %>%
  dplyr::select(site_name, NHD_RdDensWs, width_med, Dam, huc_2)

dat24_Qc <- left_join(dat24_out_cmed_pos, dat_cov_trim2)

# Next, appropriate transformations
dat24_Qc <- dat24_Qc %>%
  mutate(logQcQ2 = log10(Qc_Q2yr),
         log_width = log10(width_med)) %>%
  # Also creating the new categorical dam column to model by.
  mutate(Dam_binary = factor(case_when(
    Dam %in% c("50", "80", "95") ~ "0", # Potential effect of dams
    Dam == "0" ~ "1", # Certain effect of dams
    TRUE ~ NA))) %>%
  # and drop NAs
  drop_na(logQcQ2, NHD_RdDensWs, log_width,
          Dam_binary, huc_2)

# Ok, and making the final dataset with which to build models
# where necessary variables have already been log-transformed and
# now just need to be scaled.
dat24_Qc_brms1 <- dat24_Qc %>%
  # assigning sites to be rownames so that we can re-identify and add HUC2
  # back in once we've scaled the remaining variables
  column_to_rownames(var = "site_name") %>%
  dplyr::select(logQcQ2, NHD_RdDensWs, log_width)

dat24_Qc_brms1_scaled <- scale(dat24_Qc_brms1)

# Pull sites back in so that we can match with HUC2 values.
dat24_Qc_brms <- rownames_to_column(as.data.frame(dat24_Qc_brms1_scaled), 
                                  var = "site_name")

dat24_Qc_Dam_HUC <- dat24_Qc %>%
  dplyr::select(site_name, Dam_binary, huc_2)

dat24_Qc_brms <- left_join(dat24_Qc_brms, dat24_Qc_Dam_HUC) %>%
  dplyr::select(logQcQ2, NHD_RdDensWs, 
                Dam_binary, log_width, huc_2,
                site_name) %>%
  mutate(huc_2 = factor(huc_2))

# Fit the model.
q1_24 <- brm(logQcQ2 ~ NHD_RdDensWs + Dam_binary + log_width + (1|huc_2), 
          data = dat24_Qc_brms, family = gaussian()) # 18 sites

# Export model results.
# saveRDS(q1_24, "data_posthoc_modelfits/qcq218_brms_110323.rds")

# Examine model outputs.
summary(q1_24)

#              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept       -0.07      0.45    -0.94     0.85 1.00     1578     1571
# NHD_RdDensWs     0.03      0.35    -0.66     0.72 1.00     1838     1675
# Dam_binary1      0.01      0.60    -1.21     1.18 1.00     2221     2418
# log_width        0.13      0.36    -0.58     0.85 1.00     1880     1588

# Make summary plot.
# Pull out the data.
dat_q1_24 <- mcmc_intervals_data(q1_24,
                                  point_est = "median", # default = "median"
                                  prob = 0.66, # default = 0.5
                                  prob_outer = 0.95) # default = 0.9

# And plot.
(q24_fig_custom <- ggplot(dat_q1_24 %>%
                           filter(parameter %in% c("b_log_width",
                                                   "b_NHD_RdDensWs",
                                                   "b_Dam_binary1")) %>%
                           mutate(par_f = factor(parameter, 
                                                 levels = c("b_log_width",
                                                            "b_NHD_RdDensWs",
                                                            "b_Dam_binary1"))), 
                         aes(x = m, y = par_f, color = par_f)) +
    geom_linerange(aes(xmin = ll, xmax = hh),
                   size = 2, alpha = 0.5) +
    geom_point(size = 3) +
    vline_at(v = 0) +
    #scale_x_continuous(breaks = c(-0.4,-0.2, 0, 0.2)) +
    labs(x = "Posterior Estimates",
         y = "Predictors") +
    scale_y_discrete(labels = c("b_log_width" = "Width",
                                "b_NHD_RdDensWs" = "Roads",
                                "b_Dam_binary1" = "Dam")) +
    theme_bw() +
    scale_color_manual(values = c("#4B8FF7", "#F29F05", "#F29F05")) +
    theme(text = element_text(size = 10),
          legend.position = "none"))

# Join all posthoc tests together.
(fig24_all <- a24_fig_custom + q24_fig_custom +
  plot_annotation(tag_levels = "A"))

# And export.
# ggsave(fig24_all,
#        filename = "figures/beartooth_spring23/brms_24site_tests_110823.tiff",
#        width = 20,
#        height = 8,
#        units = "cm",
#        dpi = 300)

# Also going to create histograms to display the data coverage for contrasting
# situations (full dataset of 137 sites and selected dataset of 20 sites that
# pass model diagnostics and have complete covariate data available)

# Need to create a dataset of the original covariates.
dat_amax_orig <- dat_amax %>%
  dplyr::select(site_name, Dam, meanTemp, NHD_RdDensWs, exc_y, width_med) %>%
  # And need to add categorical dam column we modeled by.
  mutate(Dam_binary = factor(case_when(
    Dam %in% c("50", "80", "95") ~ "0", # Potential effect of dams
    Dam == "0" ~ "1", # Certain effect of dams
    TRUE ~ NA))) %>%
  # As well as a new column to color by in the histograms.
  mutate(dataset = "Full dataset")

# And a dataset of the selected sites that get fed into the model immediately
# above here.
dat_amax_select <- dat24_amax %>%
  dplyr::select(site_name, Dam, meanTemp, NHD_RdDensWs, exc_y, width_med,
                Dam_binary) %>%
  # Again add a new column to color by in the histograms.
  mutate(dataset = "Selected sites")

# Join the two together (n = 137 + n = 20).
dat_amax_twosets <- rbind(dat_amax_orig, dat_amax_20) %>%
  drop_na(meanTemp, NHD_RdDensWs, exc_y, width_med, Dam_binary)

# Dam histogram.
(fig_hist_dam <- ggplot(dat_amax_twosets %>%
                          filter(Dam != "NA"), aes(x = Dam_binary)) +
    geom_histogram(stat = "count", aes(fill = dataset), 
                   color = "black", alpha = 0.8) +
    scale_fill_manual(values = c("grey50", "grey15")) +
    labs(x = "Likelihood of Dams",
         y = "Number of Sites") +
    theme_bw() +
    theme(legend.position = "none"))

# Temperature histogram.
(fig_hist_temp <- ggplot(dat_amax_twosets, aes(x = meanTemp)) +
    geom_histogram(binwidth = 1, aes(fill = dataset), 
                   color = "black", alpha = 0.8) +
    scale_fill_manual(values = c("grey50", "grey15")) +
    labs(x = "Temperature") +
    theme_bw() +
    theme(legend.position = "none",
          axis.title.y = element_blank()))

# Roads histogram.
(fig_hist_road <- ggplot(dat_amax_twosets, aes(x = NHD_RdDensWs)) +
    geom_histogram(binwidth = 1, aes(fill = dataset), 
                   color = "black", alpha = 0.8) +
    scale_fill_manual(values = c("grey50", "grey15")) +
    labs(x = "Road Density") +
    theme_bw() +
    theme(legend.position = "none",
          axis.title.y = element_blank()))

# Exceedances histogram.
(fig_hist_exc <- ggplot(dat_amax_twosets, aes(x = exc_y)) +
    geom_histogram(binwidth = 2, aes(fill = dataset), 
                   color = "black", alpha = 0.8) +
    scale_fill_manual(values = c("grey50", "grey15")) +
    labs(x = "Annual Exceedances") +
    theme_bw() +
    theme(legend.position = "none",
          axis.title.y = element_blank()))

# Width histogram.
(fig_hist_width <- ggplot(dat_amax_twosets, aes(x = width_med)) +
    geom_histogram(binwidth = 20, aes(fill = dataset), 
                   color = "black", alpha = 0.8) +
    scale_fill_manual(values = c("grey50", "grey15")) +
    labs(x = "River Width") +
    theme_bw() +
    theme(legend.title = element_blank(),
          axis.title.y = element_blank()))

# Combine plots into one.
(fig_hist_all <- (fig_hist_dam | fig_hist_temp | fig_hist_road | fig_hist_exc | fig_hist_width) +
    plot_annotation(tag_levels = 'A'))

# And export.
# ggsave(fig_hist_all,
#        filename = "figures/beartooth_spring23/hist_24site_tests_110823.tiff",
#        width = 30,
#        height = 8,
#        units = "cm",
#        dpi = 300)

#### Reviewer 2 ####

##### Less constrained priors 6 site example #####

# These same 6 sites are also used to answer other reviewer comments above & below.

dat_in6.2 <- dat_in %>%
  filter(site_name %in% c("nwis_06795500", "nwis_02217643", 
                          "nwis_05082500", "nwis_04176500", 
                          "nwis_06893350", "nwis_07075250"))

# Split into a list by ID
l6.2 <- split(dat_in6.2, dat_in6.2$site_name)

# Export dataset for future use.
saveRDS(l6.2, "data_working/list_6sites_Qmaxnorm_SavoySL.rds")

# Rename source data
df2 <- l6.2

# Stan data prep
rstan_options(auto_write=TRUE)
# Specify number of cores
options(mc.cores=6)

# Compile necessary data
stan_data_compile <- function(x){
  data <- list(Ndays=length(x$GPP), # number of days/records
               light = x$light_rel, # relativized light (to maximum light)
               GPP = x$GPP,         # GPP estimates
               GPP_sd = x$GPP_sd,   # standard deviation of GPP estimates
               tQ = x$Q_rel,        # relativized discharge (to Qmax)
               new_e = x$new_e)     # instances of re-initialization needed
  
  return(data)
}

stan_data_l2 <- lapply(df2, function(x) stan_data_compile(x))

# Latent Biomass (Ricker population) Model
# sets initial values to help chains converge
init_Ricker2 <- function(...) {
  list(c = 0.5, s = 1.5) # values updated to match Blaszczak et al.
}

# export results
PM_outputlist_Ricker2 <- lapply(stan_data_l2,
                               function(x) stan("code/beartooth_spring23/Stan_ProductivityModel_lessinformedpriors_R1.stan",
                                                data = x, 
                                                chains = 3,
                                                iter = 5000,
                                                init = init_Ricker2,
                                                control = list(max_treedepth = 12)))

# saveRDS(PM_outputlist_Ricker2, "data_pinyon/pinyon_6rivers_output_2023_07_13.rds")

# Extract only r and c data from the model
# And use a new function to calculate median and 95%tiles.
# Obtain summary statistics using new "extract_summary" function.
# Note, this pulls diagnostics for site-level parameters.
extract_summary2 <- function(x){
  df <- x
  df1 <- summary(df,
                 pars = c("r", "c"),
                 probs = c(0.025, 0.5, 0.975))$summary # 2.5% and 97.5% percentiles
  as.data.frame(df1) %>% rownames_to_column("parameter")
}

# And now map this to the output list. 
data_out6_diags2 <- map(PM_outputlist_Ricker2, extract_summary2)

# Save this out too
saveRDS(data_out6_diags2,
       file = "data_working/pinyon_6rivers_model_rc_params_diags_071323.rds")

# Take list above and make into a df.
data_out6_diags2_df <- map_df(data_out6_diags2, ~as.data.frame(.x), .id="site_name")

dat_out6_ronly <- data_out6_diags2_df %>%
  filter(parameter == "r") %>%
  rename(r_med = `50%`)

dat_out6_conly <- data_out6_diags2_df %>%
  filter(parameter == "c") %>%
  rename(c_med = `50%`)

# plot r values #
# Filter the larger dataset.
dat_amax_6 <- dat_amax %>%
  filter(site_name %in% c("nwis_06795500", "nwis_02217643", 
                          "nwis_05082500", "nwis_04176500", 
                          "nwis_06893350", "nwis_07075250"))

# Join the two datasets.
dat_amax_6$Priors <- "Lowman et al."
dat_out6_ronly$Priors <- "Blaszczak et al."

dat_r_both_6 <- full_join(dat_amax_6, dat_out6_ronly) %>%
  # make priors a factor %>%
  mutate(Priors = factor(Priors, levels = c("Lowman et al.", "Blaszczak et al."))) %>%
  # reorder sites and add names for plotting
  mutate(Site = factor(case_when(site_name == "nwis_02217643" ~ "Parks Creek, GA",
                                 site_name == "nwis_06893350" ~ "Tomahawk Creek, KS",
                                 site_name == "nwis_07075250" ~ "S. Fork Little Red River, AR",
                                 site_name == "nwis_05082500" ~ "Red River, ND",
                                 site_name == "nwis_04176500" ~ "River Raisin, MI",
                                 site_name == "nwis_06795500" ~ "Shell Creek, NE"),
                       levels = c("S. Fork Little Red River, AR", "Tomahawk Creek, KS", 
                                  "River Raisin, MI", "Red River, ND", 
                                  "Parks Creek, GA", "Shell Creek, NE")))

(fig_r <- ggplot(dat_r_both_6, aes(x = r_med, y = Site, color = Priors)) +
    geom_point(size = 5, alpha = 0.75, position = position_dodge(width = -0.5)) +
    geom_errorbarh(aes(xmin = `2.5%`, 
                       xmax = `97.5%`), 
                   height = 0.25,
                   position = position_dodge(width = -0.5)) +
    labs(y = "Site", x = "rmax") +
    scale_color_manual(values = c("black", "grey")) +
    theme_bw()) # Lowman r values slightly lower and more constrained than Blaszczak ones

# plot c values #
# Filter the larger dataset.
dat_Qc_all_6 <- dat_Qc_all %>%
  filter(site_name %in% c("nwis_06795500", "nwis_02217643", 
                          "nwis_05082500", "nwis_04176500", 
                          "nwis_06893350", "nwis_07075250"))

# Join the two datasets.
dat_Qc_all_6$Priors <- "Lowman et al."
dat_out6_conly$Priors <- "Blaszczak et al."

dat_c_both_6 <- full_join(dat_Qc_all_6, dat_out6_conly) %>%
  # make priors a factor %>%
  mutate(Priors = factor(Priors, levels = c("Lowman et al.", "Blaszczak et al."))) %>%
  # reorder sites and add names for plotting
  mutate(Site = factor(case_when(site_name == "nwis_02217643" ~ "Parks Creek, GA",
                                 site_name == "nwis_06893350" ~ "Tomahawk Creek, KS",
                                 site_name == "nwis_07075250" ~ "S. Fork Little Red River, AR",
                                 site_name == "nwis_05082500" ~ "Red River, ND",
                                 site_name == "nwis_04176500" ~ "River Raisin, MI",
                                 site_name == "nwis_06795500" ~ "Shell Creek, NE"),
                       levels = c("S. Fork Little Red River, AR", "Tomahawk Creek, KS", 
                                  "River Raisin, MI", "Red River, ND", 
                                  "Parks Creek, GA", "Shell Creek, NE")))

(fig_c <- ggplot(dat_c_both_6, aes(x = c_med, y = Site, color = Priors)) +
    geom_point(size = 5, alpha = 0.75, position = position_dodge(width = -0.5)) +
    geom_errorbarh(aes(xmin = `2.5%`, 
                       xmax = `97.5%`), 
                   height = 0.25,
                   position = position_dodge(width = -0.5)) +
    labs(y = "Site", x = "c") +
    scale_color_manual(values = c("black", "grey")) +
    theme_bw()) # c values the same, save the AR site that converges poorly

##### c vs. s values 6 site example #####

# Using the data imported above, select for 6 sites of interest that span
# a gradient in coefficients of variation in discharge and storm events.

storm_sites6 <- c("nwis_06795500", "nwis_02217643", "nwis_05082500", "nwis_04176500", "nwis_06893350", "nwis_07075250")

dat_out_storm6 <- dat_out_df %>%
  filter(site_name %in% storm_sites6)

(fig_sc1 <- ggplot(dat_out_storm6 %>%
                   filter(site_name == "nwis_06795500"), aes(x = s, y = c)) +
  geom_point(alpha = 0.75) +
  labs(title = "Shell Creek, NE") +
  #xlim(0, 6) +
  #ylim(0, 2) +
  theme_bw())

(fig_sc2 <- ggplot(dat_out_storm6 %>%
                     filter(site_name == "nwis_02217643"), aes(x = s, y = c)) +
    geom_point(alpha = 0.75) +
    labs(title = "Parks Creek, GA") +
    #xlim(0, 6) +
    #ylim(0, 2) +
    theme_bw())

(fig_sc3 <- ggplot(dat_out_storm6 %>%
                     filter(site_name == "nwis_05082500"), aes(x = s, y = c)) +
    geom_point(alpha = 0.75) +
    labs(title = "Red River, ND") +
    #xlim(0, 6) +
    #ylim(0, 2) +
    theme_bw())

(fig_sc4 <- ggplot(dat_out_storm6 %>%
                     filter(site_name == "nwis_04176500"), aes(x = s, y = c)) +
    geom_point(alpha = 0.75) +
    labs(title = "River Raisin, MI") +
    #xlim(0, 6) +
    #ylim(0, 2) +
    theme_bw())

(fig_sc5 <- ggplot(dat_out_storm6 %>%
                     filter(site_name == "nwis_06893350"), aes(x = s, y = c)) +
    geom_point(alpha = 0.75) +
    labs(title = "Tomahawk Creek, KS") +
    #xlim(0, 6) +
    #ylim(0, 2) +
    theme_bw())

(fig_sc6 <- ggplot(dat_out_storm6 %>%
                     filter(site_name == "nwis_07075250"), aes(x = s, y = c)) +
    geom_point(alpha = 0.75) +
    labs(title = "S. Fork Little Red River, AR") + 
    #xlim(0, 6) +
    #ylim(0, 2) +
    theme_bw())

# Now, let's combine the above using patchwork.
(fig_sc <- (fig_sc1 + fig_sc2 + fig_sc3 +
              fig_sc4 + fig_sc5 + fig_sc6) +
    plot_layout(nrow = 2) +
    plot_annotation(tag_levels = 'A'))

# And export.
# ggsave(fig_sc,
#        filename = "figures/beartooth_spring23/s_vs_c_6sites_071323.jpg",
#        width = 22,
#        height = 16,
#        units = "cm")

# Also, performing a regression on c and s values to summarize how many
# sites display co-variance/potential for equifinality issues and adding
# this language to the Results section briefly.

# Take list containing all iterations of parameters and make into a df.
dat_out_df <- map_df(dat_out, ~as.data.frame(.x), .id="site_name")

dat_out_lnfits <- dat_out_df %>%
  group_by(site_name) %>%
  # fit exponential regression for each site
  #and extract only r-squared values
  summarize(lnfit_r2 = summary(lm(log(c) ~ s))$r.squared) %>%
  ungroup()

hist(dat_out_lnfits$lnfit_r2)

under_0.4_lnfits <- dat_out_lnfits %>%
  filter(lnfit_r2 < 0.4) # 149 of 181 have an R^2 less than 0.4

# End of script.
