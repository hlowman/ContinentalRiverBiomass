## Recovery of Stream Productivity following Disturbance
## Originally created: July 10, 2023
## Heili Lowman

#### READ ME ####

# The following set of scripts will walk through the steps necessary to
# prep and send data to Beartooth as well as process the model outputs.

# Much of this code has been modified from the RiverBiomass repository
# found at: https://github.com/jrblaszczak/RiverBiomass 

# Please note, the "data_XX" folders have been ignored using git.ignore.
# Links to the raw data sets are provided in the 01_NWIS_RiverSelection.R file.

# If you are accessing the code via GitHub, these will need to be 
# downloaded and added to a folder of the appropriate name prior to running the 
# code below.

# This file will address remaining analyses to support responses to reviewer
# comments.

# The following script was run using the Pinyon server at the University of 
# Nevada Reno for speed when re-running the full model at 6 selected sites.

#### Setup ####

# Load necessary packages.
lapply(c("tidybayes", "brms", "tidyverse", "lubridate", 
         "data.table", "GGally", "plotly", "bayesplot",
         "multcomp", "patchwork", "bayesplot", "shinystan",
         "modelsummary", "here", "nlme", "loo", "parallel",
         "tidyverse", "rstan", "devtools"), 
       require, character.only=T)

#### Data ####

# Import necessary datasets.

# First, the model outputs.
dat_out <- readRDS("data_working/beartooth_181rivers_model_params_all_iterations_050823.rds")

# And, the original data used in modeling.
dat_in <- readRDS("data_working/df_181sites_Qmaxnorm_SavoySL.rds")

# Then, the data for the maximum accrual (amax) models.
dat_amax <- readRDS("data_working/amax_covariates_152sites_070523.rds")

# Next, the data for the Qc:Q2yr models.
dat_Qc <- readRDS("data_working/Qc_covariates_138sites_070523.rds") %>%
  mutate(Qc_Q2yr = Qc/RI_2yr_Q_cms)

# And Qc data at *all* sites.
dat_Qc_all <- readRDS("data_working/QcQ2_unfiltered_181sites_051023.rds") %>%
  mutate(Qc_Q2yr = Qc/RI_2yr_Q_cms)

# A dataset with long names for IDing purposes.
dat_names <- readRDS("data_working/NWIS_Info_181riv_HUC2_df_050923.rds") %>%
  dplyr::select(site_no, station_nm)

# And all covariate data at all sites.
dat_cov <- readRDS("data_working/covariate_data_181sites_070523.rds")

# And 10yr flood values for all sites generated by the script
# "Discharge_TS_10yearflood_207riv.R"
dat_10yr <- read_csv("data_working/RI_10yr_flood_206riv.csv")

# Import data used to fit posthoc models.
dat_a1 <- readRDS("data_posthoc_modelfits/accrual_datin_100323.rds")
dat_q1 <- readRDS("data_posthoc_modelfits/qc_datin_100323.rds")

# As well as posthoc model fits from step 13 script.
a1 <- readRDS("data_posthoc_modelfits/accrual_brms_070623.rds")
q1 <- readRDS("data_posthoc_modelfits/qcq2_brms_070623.rds")

#### Reviewer 1 ####

##### Normalizing instead to Q10 #####

# First, need to combine raw input data (dat_in) with available estimates
# of the 10 yr flood at the 6 sites at which I will be re-running the model.

# These same 6 sites are also used to answer other reviewer comments below.

dat_in6 <- dat_in %>%
  filter(site_name %in% c("nwis_06795500", "nwis_02217643", 
                          "nwis_05082500", "nwis_04176500", 
                          "nwis_06893350", "nwis_07075250"))

dat_in6 <- left_join(dat_in6, dat_10yr, by = c("site_name"))

# Create new column with discharge (Q) relativized to the 10yr flood 
# rather than the maximum Q in a given record.

dat_in6$Q_relQ10 <- (dat_in6$Q/dat_in6$RI_10yr_Q_cms)

# Split into a list by ID
l6 <- split(dat_in6, dat_in6$site_name)

# Export dataset for future use.
saveRDS(l6, "data_working/list_6sites_Q10norm_SavoySL.rds")

# Rename source data
df <- l6

# Stan data prep
rstan_options(auto_write=TRUE)
# Specify number of cores
options(mc.cores=6)

# Compile necessary data
stan_data_compile <- function(x){
  data <- list(Ndays=length(x$GPP), # number of days/records
               light = x$light_rel, # relativized light (to maximum light)
               GPP = x$GPP,         # GPP estimates
               GPP_sd = x$GPP_sd,   # standard deviation of GPP estimates
               tQ = x$Q_relQ10,     # relativized discharge (to Q_10yr)
               new_e = x$new_e)     # instances of re-initialization needed
  
  return(data)
}

stan_data_l <- lapply(df, function(x) stan_data_compile(x))

# Latent Biomass (Ricker population) Model
# sets initial values to help chains converge
init_Ricker <- function(...) {
  list(r = 0.2, lambda = -0.03, c = 0.5, s = 1.5) # values to match priors
}

## export results
PM_outputlist_Ricker <- lapply(stan_data_l,
                               function(x) stan("code/beartooth_spring23/Stan_ProductivityModel.stan",
                                           data = x, 
                                           chains = 3,
                                           iter = 5000,
                                           init = init_Ricker,
                                           control = list(max_treedepth = 12)))

# saveRDS(PM_outputlist_Ricker, "data_pinyon/pinyon_6rivers_output_2023_07_12.rds")

# Extract only c and s data from the model
# Going to create a function to pull out all iterations.
extract_cs <- function(df){
  rstan::extract(df, c("c", "s"))
}

data_out6_cs <- map(PM_outputlist_Ricker, extract_cs)

# Save this out too
# saveRDS(data_out6_cs,
#        file = "data_working/pinyon_6rivers_model_cs_params_all_iterations_071223.rds")

# And now use a new function to calculate median and 95%tiles.
# Obtain summary statistics using new "extract_summary" function.
# Note, this pulls diagnostics for site-level parameters.
extract_summary <- function(x){
  df <- x
  df1 <- summary(df,
                 pars = c("c", "s"),
                 probs = c(0.025, 0.5, 0.975))$summary # 2.5% and 97.5% percentiles
  as.data.frame(df1) %>% rownames_to_column("parameter")
}

# And now map this to the output list. 
data_out6_diags <- map(PM_outputlist_Ricker, extract_summary)

# Save this out too
# saveRDS(data_out6_diags,
#        file = "data_working/pinyon_6rivers_model_cs_params_diags_071323.rds")

# Take list above and make into a df.
data_out6_diags_df <- map_df(data_out6_diags, ~as.data.frame(.x), .id="site_name") %>%
  # and for now, I'm going to focus solely on "c" values
  filter(parameter == "c")

dat_QcQ10_6 <- left_join(data_out6_diags_df, dat_10yr) %>%
  rename(c_med = `50%`)

# Calculate Qc in cms.
dat_QcQ10_6 <- dat_QcQ10_6 %>%
  mutate(Qc = c_med * RI_10yr_Q_cms,
         Qc2.5 = `2.5%` * RI_10yr_Q_cms,
         Qc97.5 = `97.5%` * RI_10yr_Q_cms)

# Filter the larger dataset that includes values from when Q was normalized
# to the maximum Q of the dataset.
dat_QcQmax_6 <- dat_Qc_all %>%
  filter(site_name %in% c("nwis_06795500", "nwis_02217643", 
         "nwis_05082500", "nwis_04176500", 
         "nwis_06893350", "nwis_07075250"))

# Join the two datasets.
dat_QcQ10_6$Normalization <- "Q10"
dat_QcQmax_6$Normalization <- "Qmax"

dat_Qc_both_6 <- full_join(dat_QcQ10_6, dat_QcQmax_6) %>%
  # make norm a factor %>%
  mutate(Normalization = factor(Normalization, levels = c("Qmax", "Q10"))) %>%
  # reorder sites and add names for plotting
  mutate(Site = factor(case_when(site_name == "nwis_02217643" ~ "Parks Creek, GA",
                          site_name == "nwis_06893350" ~ "Tomahawk Creek, KS",
                          site_name == "nwis_07075250" ~ "S. Fork Little Red River, AR",
                          site_name == "nwis_05082500" ~ "Red River, ND",
                          site_name == "nwis_04176500" ~ "River Raisin, MI",
                          site_name == "nwis_06795500" ~ "Shell Creek, NE"),
                       levels = c("S. Fork Little Red River, AR", "Tomahawk Creek, KS", 
                                  "River Raisin, MI", "Red River, ND", 
                                  "Parks Creek, GA", "Shell Creek, NE")))

(fig_c <- ggplot(dat_Qc_both_6, aes(x = Qc, y = Site, color = Normalization)) +
    geom_point(size = 5, alpha = 0.75, position = position_dodge(width = -0.5)) +
    geom_errorbarh(aes(xmin = Qc2.5, 
                       xmax = Qc97.5), 
                   height = 0.25,
                   position = position_dodge(width = -0.5)) +
    labs(y = "Site", x = "Qc (cms)") +
    scale_color_manual(values = c("black", "grey")) +
    theme_bw())

# And export.
# ggsave(fig_c,
#        filename = "figures/beartooth_spring23/c_q10_vs_qmaxnorm_fig.jpg",
#        width = 16,
#        height = 10,
#        units = "cm")

##### Pred GPP in the South Fork Iowa ######

# First, need to create the function for predicting GPP.
PM_Ricker <- function(r, lambda, s, c, sig_p, sig_o, df) {
  
  ## Data
  Ndays <- length(df$GPP)
  GPP <- df$GPP
  GPP_sd <- df$GPP_sd
  light <- df$light_rel
  tQ <- df$Q_rel # discharge standardized to max value
  new_e <- df$new_e
  
  ## Vectors for model output of P, B, pred_GPP
  P <- numeric(Ndays)
  P[1] <- 1
  for(i in 2:length(tQ)){
    P[i] = exp(-exp(s*100*(tQ[i] - c)))
  }
  
  B <- numeric(Ndays)
  B[1] <- log(GPP[1]/light[1])
  pred_GPP <- numeric(Ndays)
  pred_GPP[1] <- light[1]*exp(B[1])
  
  ## Process Model
  for(j in 2:Ndays){
    # adding in section for my re-initialization functionality
    if (new_e[j]==1) {
      
      B[j] ~ MCMCglmm::rtnorm(1, mean = log(GPP[j]/light[j])) }
    
    else {
      
      B[j] <- MCMCglmm::rtnorm(1, mean = (B[j-1] + r + lambda*exp(B[j-1]))*P[j],
                               sd = sig_p, upper = 5) }
    
  }
  
  for(i in 2:Ndays){
    pred_GPP[i] <- MCMCglmm::rtnorm(1, mean = light[i]*exp(B[i]), 
                                    sd = sig_o, lower = 0.01)
  }
  
  return(pred_GPP)
}

# Next, need to write the function with which to perform the simulation.
Ricker_sim_fxn <- function(y, x){
  # identify data
  output <- y # Teton/stan output
  df <- x # original data input
  
  # extracted parameters from STAN output already
  pars <- output
  
  # create empty matrix with days of GPP x length of iterations to receive values
  simmat <- matrix(NA, length(df$GPP), length(unlist(pars$sig_p)))
  rmsemat <- matrix(NA, length(df$GPP), 1)
  
  # simulate pred_GPP holding a parameter set for a given iteration constant
  # and then predict forward for a site's timeseries (i.e., length(df$GPP))
  for(i in 1:length(pars$r)){
    simmat[,i] <- PM_Ricker(pars$r[i], pars$lambda[i], pars$s[i], pars$c[i], pars$sig_p[i], pars$sig_p[i], df)
    rmsemat[i] <- sqrt(sum((simmat[,i] - df$GPP)^2)/length(df$GPP))
  }
  
  l <- list(simmat, rmsemat)
  return(l)
  
}

# Adding the nRMSE calculation into the function above didn't play nicely with
# the list that existed, so calculating outside instead.
nRMSE_fxn <- function(df, df_orig){
  
  # Calculate the mean RMSE value for each site.
  nRMSE <- mean(df)/(max(df_orig$GPP) - min(df_orig$GPP))
  
}

# Take list containing all iterations of parameters and make into a df.
dat_out_df <- map_df(dat_out, ~as.data.frame(.x), .id="site_name")

# Trimming input and output datasets for the site of interest.
dat_out1df <- dat_out_df %>%
  filter(site_name %in% c("nwis_05451210"))

dat_in1df <- dat_in %>%
  filter(site_name %in% c("nwis_05451210"))

dat_in1 <- split(dat_in1df, dat_in1df$site_name)

# Re-simulating using all output iterations. Started ~1:10, Ended ~??
Ricker_sim1 <- Ricker_sim_fxn(dat_out1df, dat_in1df)

# Exporting to save progress.
# saveRDS(Ricker_sim1, "data_working/Ricker_sim_1site_081623.rds")

# And for each day, I would like to calculate 2.5%, 50%, and 97.5%tiles.

# Going to pull out just the predicted GPP values.
# So, making a list of odd numbers to pull out predGPP values.
data_1site_gpp <- Ricker_sim1[1]

# Calculate median and confidence intervals
quantile25 <- function(x){quantile(x, probs = 0.025, na.rm = TRUE)}
quantile975 <- function(x){quantile(x, probs = 0.975, na.rm = TRUE)}

pred_gpp1 <- lapply(data_1site_gpp, 
                    function(x) cbind(apply(x, 1, median),
                                      apply(x, 1, quantile25),
                                      apply(x, 1, quantile975)))

# Pull out original GPP values used and sequence #s (for plotting)
orig_gpp_date1 <- lapply(dat_in1, function(x) x %>% dplyr::select(date, GPP, seq,
                                                                  Q, Q_rel))

# Add names to confidence interval lists
my_names <- c("nwis_05451210")

names(pred_gpp1) <- my_names

pred_gpp1 <- lapply(pred_gpp1, function(x) as.data.frame(x) %>% 
                      rename("Median" = "V1",
                             "q2.5" = "V2",
                             "q97.5" = "V3")) # OMG YAY!!!!

# Bind into a single dataframe
keys <- unique(c(names(orig_gpp_date1), names(pred_gpp1)))
df_pred1 <- setNames(Map(cbind, orig_gpp_date1[keys], pred_gpp1[keys]), keys)

# And finally, calculate the normalized RMSE.
rmse1 <- Ricker_sim1[2]

nRMSE_1site <- mapply(nRMSE_fxn, rmse1, dat_in1)

# And plot
# South Fork Iowa River, IA
# First panel GPP
(gpp_plot_rmse1a <- ggplot(df_pred1$nwis_05451210 %>%
                             filter(date < ymd(as.character("2008-12-31"))), 
                           aes(date, GPP)) +
    geom_point(size = 2, color = "#303018") +
    geom_line(aes(date, Median), color = "#609048", linewidth = 1.2) +
    labs(y = expression('GPP (g '*~O[2]~ m^-2~d^-1*')'),
        x = "Date") +
    scale_x_date(date_labels = "%b %Y", date_breaks = "3 months") +
    geom_ribbon(aes(ymin = q2.5,
                    ymax = q97.5),
                fill = "#90A860", alpha = 0.3) +
    annotate(geom = "text", x = date("2008-09-01"), y = 17.5,
             label = paste("nRMSE = ", round(nRMSE_1site[1], 
                           digits = 2)), size = 4) +
    theme_bw() +
    theme(legend.position = "none",
          title = element_text(size = 10),
          axis.title.x = element_text(size=10),
          axis.text.x = element_text(size=10),
          axis.text.y = element_text(size=10),
          axis.title.y = element_text(size=10)))

# First panel Q
(gpp_plot_rmse1a_q <- ggplot(df_pred1$nwis_05451210 %>%
                             filter(date < ymd(as.character("2008-12-31"))), 
                           aes(date, Q)) +
    geom_line(color = "#5792CC", linewidth = 1.2) +
    geom_hline(yintercept = 41.6, size = 1) +
    geom_hline(yintercept = 28.8, size = 1, linetype = "dashed") +
    labs(y = expression('Discharge ('*m^3~s^-1*')'),
         x = "Date") +
    scale_x_date(date_labels = "%b %Y", date_breaks = "3 months") +
    ylim(0, 170) +
    theme_bw() +
    theme(legend.position = "none",
          title = element_text(size = 10),
          axis.title.x = element_text(size=10),
          axis.text.x = element_text(size=10),
          axis.text.y = element_text(size=10),
          axis.title.y = element_text(size=10)))

# Second panel
(gpp_plot_rmse1b <- ggplot(df_pred1$nwis_05451210 %>%
                             filter(date > ymd(as.character("2009-12-31"))), 
                           aes(date, GPP)) +
    geom_point(size = 2, color = "#303018") +
    geom_line(aes(date, Median), color = "#609048", linewidth = 1.2) +
    labs(y = expression('GPP (g '*~O[2]~ m^-2~d^-1*')'),
         x = "Date") +
    scale_x_date(date_labels = "%b %Y", date_breaks = "3 months") +
    geom_ribbon(aes(ymin = q2.5,
                    ymax = q97.5),
                fill = "#90A860", alpha = 0.3) +
    theme_bw() +
    theme(legend.position = "none",
          title = element_text(size = 10),
          axis.title.x = element_text(size=10),
          axis.text.x = element_text(size=10),
          axis.text.y = element_text(size=10),
          axis.title.y = element_text(size=10)))

# Second panel Q
(gpp_plot_rmse1b_q <- ggplot(df_pred1$nwis_05451210 %>%
                               filter(date > ymd(as.character("2009-12-31"))), 
                             aes(date, Q)) +
    geom_line(color = "#5792CC", linewidth = 1.2) +
    geom_hline(yintercept = 41.6, size = 1) +
    geom_hline(yintercept = 28.8, size = 1, linetype = "dashed") +
    labs(y = expression('Discharge ('*m^3~s^-1*')'),
         x = "Date") +
    scale_x_date(date_labels = "%b %Y", date_breaks = "3 months") +
    ylim(0, 170) +
    theme_bw() +
    theme(legend.position = "none",
          title = element_text(size = 10),
          axis.title.x = element_text(size=10),
          axis.text.x = element_text(size=10),
          axis.text.y = element_text(size=10),
          axis.title.y = element_text(size=10)))

# Now, let's combine the above using patchwork.
(fig_SFIR <- (gpp_plot_rmse1a + gpp_plot_rmse1b +
                gpp_plot_rmse1a_q + gpp_plot_rmse1b_q) +
    plot_layout(nrow = 2) +
    plot_annotation(tag_levels = 'A'))

# And export.
# ggsave(fig_SFIR,
#        filename = "figures/beartooth_spring23/predGPP_SForkIowa_081723.jpg",
#        width = 30,
#        height = 15,
#        units = "cm")

##### New Site Selection for SI Figure #####

# Need to identify 16 sites that span time series length, land cover type,
# light, and flow regimes to address reviewer's comments re: heterogeneity
# of results across site types.

# Create site groupings for land cover.
dev <- c("urban", "agricultural")
undev <- c("grassland", "forested", "water", "wetland")

# Plot using most conservative dataset.
dat_Qc <- dat_Qc %>%
  
  mutate(my_groups = factor(case_when(total_days > 870 & LU_category %in% dev &
                                        cvQ <= 1 & meanLight > 24 ~ "Long_Dev_Steady_Light",
                                      total_days > 870 & LU_category %in% dev &
                                        cvQ <= 1 & meanLight <= 24 ~ "Long_Dev_Steady_Dark",
                                      total_days > 870 & LU_category %in% dev &
                                        cvQ > 1 & meanLight > 24 ~ "Long_Dev_Turb_Light",
                                      total_days > 870 & LU_category %in% dev &
                                        cvQ > 1 & meanLight <= 24 ~ "Long_Dev_Turb_Dark",
                                      total_days > 870 & LU_category %in% undev &
                                        cvQ <= 1 & meanLight > 24 ~ "Long_Undev_Steady_Light",
                                      total_days > 870 & LU_category %in% undev &
                                        cvQ <= 1 & meanLight <= 24 ~ "Long_Undev_Steady_Dark",
                                      total_days > 870 & LU_category %in% undev &
                                        cvQ > 1 & meanLight > 24 ~ "Long_Undev_Turb_Light",
                                      total_days > 870 & LU_category %in% undev &
                                        cvQ > 1 & meanLight <= 24 ~ "Long_Undev_Turb_Dark",
                                      total_days <= 870 & LU_category %in% dev &
                                        cvQ <= 1 & meanLight > 24 ~ "Short_Dev_Steady_Light",
                                      total_days <= 870 & LU_category %in% dev &
                                        cvQ <= 1 & meanLight <= 24 ~ "Short_Dev_Steady_Dark",
                                      total_days <= 870 & LU_category %in% dev &
                                        cvQ > 1 & meanLight > 24 ~ "Short_Dev_Turb_Light",
                                      total_days <= 870 & LU_category %in% dev &
                                        cvQ > 1 & meanLight <= 24 ~ "Short_Dev_Turb_Dark",
                                      total_days <= 870 & LU_category %in% undev &
                                        cvQ <= 1 & meanLight > 24 ~ "Short_Undev_Steady_Light",
                                      total_days <= 870 & LU_category %in% undev &
                                        cvQ <= 1 & meanLight <= 24 ~ "Short_Undev_Steady_Dark",
                                      total_days <= 870 & LU_category %in% undev &
                                        cvQ > 1 & meanLight > 24 ~ "Short_Undev_Turb_Light",
                                      total_days <= 870 & LU_category %in% undev &
                                        cvQ > 1 & meanLight <= 24 ~ "Short_Undev_Turb_Dark",
                                      TRUE ~ NA),
                            levels = c("Long_Dev_Steady_Light",
                                       "Long_Dev_Steady_Dark",
                                       "Long_Dev_Turb_Light",
                                       "Long_Dev_Turb_Dark",
                                       "Long_Undev_Steady_Light",
                                       "Long_Undev_Steady_Dark",
                                       "Long_Undev_Turb_Light",
                                       "Long_Undev_Turb_Dark",
                                       "Short_Dev_Steady_Light",
                                       "Short_Dev_Steady_Dark",
                                       "Short_Dev_Turb_Light",
                                       "Short_Dev_Turb_Dark",
                                       "Short_Undev_Steady_Light",
                                       "Short_Undev_Steady_Dark",
                                       "Short_Undev_Turb_Light",
                                       "Short_Undev_Turb_Dark")))

dat_r_Qc_plus <- left_join(dat_Qc, dat_amax %>% dplyr::select(site_name, yield_med))

(viz_fig <- ggplot(dat_r_Qc_plus, aes(x = yield_med, y = Qc_Q2yr)) +
    geom_jitter(alpha = 0.8, size = 3, 
                aes(color = my_groups, text = site_name)) +
    scale_y_log10() +
    scale_x_log10() +
    labs(x = "Maximum Daily Accrual",
         y = "Qc:Q2yr",
         color = "Groupings") +
    theme_bw())

# ggsave(viz_fig,
#        filename = "figures/beartooth_spring23/Sites_16Groups_071123.jpg",
#        width = 15,
#        height = 10,
#        units = "cm") # n = 138

(viz_plotly <- ggplotly(viz_fig))

# Sites I will use for the new figure are: nwis_13213000, nwis_05435950,
# nwis_03219500, nwis_01648010, nwis_04137500, nwis_14211010, nwis_07109500,
# nwis_11044000, nwis_05057200, nwis_12102075, nwis_05451210, nwis_06893970,
# nwis_08447300, nwis_02217643, nwis_04059500, nwis_03538830

# Note, the revised version of the Supplementary figure displaying predicted 
# GPP at multiple sites can be found in the "14_Appendix_figures.R" script.

##### Effects of Variation in TS Length #####

###### Posthoc model residuals ######

# First, I will examine how the post-hoc model residuals compare to the
# length of each of the time series.

# Create a dataset with only site-names and lengths of ts
ts_lengths <- dat_in %>%
  group_by(site_name) %>%
  summarize(days = n()) %>% # count the number of rows/days
  ungroup()

# Extract residuals for first amax posthoc model.
ra1 <- residuals(a1,
                 method = "posterior_predict",
                 summary = TRUE)

# Generate dataframe initially used to fit accrual posthoc model.
dat_a1_144 <- tidyr::drop_na(dat_a1)

# Join with ts length.
dat_a1_144 <- left_join(dat_a1_144, ts_lengths)

# And join with residuals
dat_a1_144 <- cbind(dat_a1_144, ra1)

# Plot residuals of accrual posthoc model vs. time series length
(fig_resids_a <- ggplot(dat_a1_144, aes(x = days, y = Estimate)) +
  geom_point(size = 3, alpha = 0.75) +
  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.75) +
  labs(x = "Time Series Length (days)",
       y = "Accrual Model Residual Estimate") +
  theme_bw())

# Extract residuals for second QcQ2yr posthoc model.
rq1 <- residuals(q1,
                 method = "posterior_predict",
                 summary = TRUE)

# Generate dataframe initially used to fit accrual posthoc model.
dat_q1_130 <- tidyr::drop_na(dat_q1)

# Join with ts length.
dat_q1_130 <- left_join(dat_q1_130, ts_lengths)

# And join with residuals
dat_q1_130 <- cbind(dat_q1_130, rq1)

# Plot residuals of accrual posthoc model vs. time series length
(fig_resids_q <- ggplot(dat_q1_130, aes(x = days, y = Estimate)) +
    geom_point(size = 3, alpha = 0.75) +
    geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.75) +
    labs(x = "Time Series Length (days)",
         y = "Critical Disturbance Threshold\nModel Residual Estimate") +
    theme_bw())

# Join figures together.
fig_resids <- fig_resids_a + fig_resids_q

# And export.
# ggsave(fig_resids,
#        filename = "figures/beartooth_spring23/resids_and_ts_length_fig.jpg",
#        width = 20,
#        height = 10,
#        units = "cm")

# And as an added gut check, I will re-run the model fits to examine
# time series length as covariate.

# Join raw dataset with time series lengths.
dat_amax_ts <- left_join(dat_amax, ts_lengths)

dat_amax_ts <- dat_amax_ts %>%
  mutate(log_yield = log10(yield_med)) %>% # log-transform yield
  mutate(log_width = log10(width_med)) %>% # log-transform width
  mutate(log_days = log10(days)) %>% # log-transform length
  # Creating a new categorical dam column to model by.
  mutate(Dam_binary = factor(case_when(
    Dam %in% c("50", "80", "95") ~ "0", # Potential = 5-50%
    Dam == "0" ~ "1", # Certain = 100%
    TRUE ~ NA)))

# Necessary variables have already been log-transformed and
# now just need to be scaled.
dat_amax_ts_brms1 <- dat_amax_ts %>%
  # assigning sites to be rownames so that we can re-identify and add HUC2
  # and Dams back in once we've scaled the remaining variables
  column_to_rownames(var = "site_name") %>%
  dplyr::select(log_yield, meanTemp, NHD_RdDensWs, 
                log_width, exc_y, log_days)

dat_amax_ts_brms1_scaled <- scale(dat_amax_ts_brms1) # scale variables

# Pull sites back in so that we can match with HUC2 values.
dat_amax_ts_brms <- rownames_to_column(as.data.frame(dat_amax_ts_brms1_scaled), 
                                    var = "site_name")

dat_amax_ts_Dam_HUC <- dat_amax_ts %>%
  dplyr::select(site_name, Dam_binary, huc_2)

dat_amax_ts_brms <- full_join(dat_amax_ts_brms, dat_amax_ts_Dam_HUC) %>%
  dplyr::select(log_yield, meanTemp, NHD_RdDensWs, 
                Dam_binary, log_width, exc_y, huc_2,
                log_days, site_name) %>%
  mutate(huc_2 = factor(huc_2))

# Model fit for accrual.
a1_ts <- brm(log_yield ~ log_width + NHD_RdDensWs +
               Dam_binary + meanTemp + exc_y + 
               log_days + (1|huc_2), 
          data = dat_amax_ts_brms, family = gaussian())

# Examine model outputs.
summary(a1_ts)

#              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept        0.24      0.16    -0.06     0.56 1.00     2653     1823
# log_width        0.48      0.11     0.27     0.69 1.00     3567     3047
# NHD_RdDensWs     0.02      0.10    -0.19     0.22 1.00     3740     3305
# Dam_binary1     -0.40      0.15    -0.70    -0.09 1.00     5746     3163
# meanTemp         0.04      0.10    -0.15     0.24 1.00     3010     2770
# exc_y            0.17      0.07     0.03     0.32 1.00     5397     3392
# log_days         0.14      0.09    -0.03     0.31 1.00     4017     3148

# All covariates remain as before, and no. of days is not significant.

mcmc_plot(a1_ts)

# And now for other model fit (Qc:Q2yr).
# Join raw dataset with time series lengths.
dat_Qc_ts <- left_join(dat_Qc, ts_lengths)

# Going to log transform QcQ2yr too.
dat_Qc_ts <- dat_Qc_ts %>%
  mutate(logQcQ2 = log10(Qc_Q2yr),
         log_width = log10(width_med),
         log_days = log10(days)) %>%
  # Creating the new categorical dam column to model by.
  mutate(Dam_binary = factor(case_when(
    Dam %in% c("50", "80", "95") ~ "0", # Potential effect of dams
    Dam == "0" ~ "1", # Certain effect of dams
    TRUE ~ NA)))

# Necessary variables have already been log-transformed and
# now just need to be scaled.
dat_Qc_ts_brms1 <- dat_Qc_ts %>%
  # assigning sites to be rownames so that we can re-identify and add HUC2
  # back in once we've scaled the remaining variables
  column_to_rownames(var = "site_name") %>%
  dplyr::select(logQcQ2, NHD_RdDensWs, log_width, log_days)

dat_Qc_ts_brms1_scaled <- scale(dat_Qc_ts_brms1)

# Pull sites back in so that we can match with HUC2 values.
dat_Qc_ts_brms <- rownames_to_column(as.data.frame(dat_Qc_ts_brms1_scaled), 
                                  var = "site_name")

dat_Qc_ts_Dam_HUC <- dat_Qc_ts %>%
  dplyr::select(site_name, Dam_binary, huc_2)

dat_Qc_ts_brms <- left_join(dat_Qc_ts_brms, dat_Qc_ts_Dam_HUC) %>%
  dplyr::select(logQcQ2, NHD_RdDensWs, 
                Dam_binary, log_width, huc_2,
                log_days, site_name) %>%
  mutate(huc_2 = factor(huc_2))

# Model fit for critical disturbance threshold normalized to 2yr flood.
q1_ts <- brm(logQcQ2 ~ NHD_RdDensWs + Dam_binary + 
               log_width + log_days + (1|huc_2), 
          data = dat_Qc_ts_brms, family = gaussian()) 

# Examine model results.
summary(q1_ts)

#              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept        0.20      0.20    -0.17     0.62 1.00     1586     1675
# NHD_RdDensWs    -0.08      0.12    -0.31     0.16 1.00     3438     3060
# Dam_binary1     -0.21      0.18    -0.56     0.14 1.00     4751     3101
# log_width        0.03      0.12    -0.20     0.26 1.00     3401     3205
# log_days         0.11      0.10    -0.07     0.29 1.00     4234     2798

# As above, all covariates remain as before, and no. of days is 
# not significant.

mcmc_plot(q1_ts)

###### Resampling approach ######

# For consistency, I will aim to use sites included in the Appendix Figure.

# First, I will investigate which of those sites have the longest records,
# and select 3.

my16sites <- c("nwis_13213000", "nwis_05435950", "nwis_03219500", 
               "nwis_01648010", "nwis_04137500", "nwis_14211010", 
               "nwis_07109500", "nwis_11044000", "nwis_05057200", 
               "nwis_12102075", "nwis_05451210", "nwis_06893970",
               "nwis_08447300", "nwis_02217643", "nwis_04059500", 
               "nwis_03538830")

dat_in16 <- dat_in %>%
  filter(site_name %in% my16sites)

dat_in16_days <- dat_in16 %>%
  group_by(site_name) %>%
  summarize(ndays = n()) %>%
  ungroup()

# These sites appear to be good candidates for the TS length analysis:
# nwis_04137500 - 2788 days or ~ 7.6 years
# nwis_07109500 - 2376 days or ~ 6.5 years
# nwis_14211010 - 1603 days or ~ 4.4 years

# Select these three sites and then further split them into 5 timeframes each:
# 3 months, 6 months, 1 year, 2 years, and 4 years of data
# to compare parameter estimates when full timeseries is used.

dat_in_04137500 <- dat_in %>%
  filter(site_name == "nwis_04137500")

# select random indexes (to the nearest day) of timeframes to sample
# with max values being based on the end of the data length
mo3 <- round(runif(1, min=1, max=2698)) # 1936
mo6 <- round(runif(1, min=1, max=2608)) # 1034
yr1 <- round(runif(1, min=1, max=2423)) # 1393
yr2 <- round(runif(1, min=1, max=2058)) # 668
yr4 <- round(runif(1, min=1, max=1328)) # 584

# select data according to these random indices
# and add new columns for relative L and Q
dat_in_04137500_mo3 <- dat_in_04137500[mo3:(mo3+90),] %>% 
  mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
         Q_rel = Q/max(Q, na.rm = TRUE),
         sample = "3 months")
dat_in_04137500_mo6 <- dat_in_04137500[mo6:(mo6+180),] %>% 
  mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
         Q_rel = Q/max(Q, na.rm = TRUE),
         sample = "6 months")
dat_in_04137500_yr1 <- dat_in_04137500[yr1:(yr1+365),] %>% 
  mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
         Q_rel = Q/max(Q, na.rm = TRUE),
         sample = "1 year")
dat_in_04137500_yr2 <- dat_in_04137500[yr2:(yr2+730),] %>% 
  mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
         Q_rel = Q/max(Q, na.rm = TRUE),
         sample = "2 years")
dat_in_04137500_yr4 <- dat_in_04137500[yr4:(yr4+1460),] %>% 
  mutate(light_rel = PAR_surface/max(PAR_surface, na.rm = TRUE),
         Q_rel = Q/max(Q, na.rm = TRUE),
         sample = "4 years")

# join into a single dataframe
dat_in3 <- rbind(dat_in_04137500_mo3, dat_in_04137500_mo6)
dat_in3 <- rbind(dat_in3, dat_in_04137500_yr1)
dat_in3 <- rbind(dat_in3, dat_in_04137500_yr2)
dat_in3 <- rbind(dat_in3, dat_in_04137500_yr4)

# Split into a list by ID
l3 <- split(dat_in3, dat_in3$sample)

# Export dataset for future use.
saveRDS(l3, "data_working/list_1site_5ts_Qmaxnorm_SavoySL.rds")

# Rename source data
df <- l3

# Stan data prep
rstan_options(auto_write=TRUE)
# Specify number of cores
options(mc.cores=6)

# Compile necessary data
stan_data_compile <- function(x){
  data <- list(Ndays=length(x$GPP), # number of days/records
               light = x$light_rel, # relativized light (to maximum light)
               GPP = x$GPP,         # GPP estimates
               GPP_sd = x$GPP_sd,   # standard deviation of GPP estimates
               tQ = x$Q_rel,        # relativized discharge (to maximum Q)
               new_e = x$new_e)     # instances of re-initialization needed
  
  return(data)
}

stan_data_l <- lapply(df, function(x) stan_data_compile(x))

# Latent Biomass (Ricker population) Model
# sets initial values to help chains converge
init_Ricker <- function(...) {
  list(r = 0.2, lambda = -0.03, c = 0.5, s = 1.5) # values to match priors
}

## export results - started at 11:15am
PM_outputlist_Ricker <- lapply(stan_data_l,
                               function(x) stan("code/beartooth_spring23/Stan_ProductivityModel.stan",
                                                data = x, 
                                                chains = 3,
                                                iter = 5000,
                                                init = init_Ricker,
                                                control = list(max_treedepth = 12)))

# saveRDS(PM_outputlist_Ricker, "data_pinyon/pinyon_1river_5ts_output_2023_09_30.rds")

# Going to create a function to pull out all iterations.
extract_params <- function(df){
  rstan::extract(df, c("r", "lambda", "c", "s"))
}

data_out_04137500 <- map(PM_outputlist_Ricker, extract_params)

# Save this out too
# saveRDS(data_out_04137500,
#        file = "data_working/pinyon_1river_5ts_params_all_iterations_093023.rds")

# And now use a new function to calculate median and 95%tiles.
# Obtain summary statistics using new "extract_summary" function.
# Note, this pulls diagnostics for site-level parameters.
extract_summary <- function(x){
  df <- x
  df1 <- summary(df,
                 pars = c("r", "lambda", "c", "s"),
                 probs = c(0.025, 0.5, 0.975))$summary
  as.data.frame(df1) %>% rownames_to_column("parameter")
}

# And now map this to the output list. 
data_out_04137500_diags <- map(PM_outputlist_Ricker, extract_summary)

# Save this out too
# saveRDS(data_out_04137500_diags,
#        file = "data_working/pinyon_1river_5ts_params_diags_093023.rds")

# Take list above and make into a df.
data_out_04137500_c <- map_df(data_out_04137500_diags, 
                                     ~as.data.frame(.x), .id="sample") %>%
  # and for now, I'm going to focus solely on "r" and "c" values
  filter(parameter %in% c("c")) %>%
  # and rename to match the other dataset
  rename(c_med = `50%`)

# Filter the larger dataset that includes values from the same site.
dat_out_04137500_orig_c <- dat_Qc_all %>%
  filter(site_name == "nwis_04137500")

# Join the two datasets.
dat_out_04137500_orig_c$sample <- "7.5 years"

dat_c_both <- full_join(dat_out_04137500_orig_c, data_out_04137500_c) %>%
  # make norm a factor %>%
  mutate(sample = factor(sample, levels = c("3 months", "6 months",
                                            "1 year", "2 years", "4 years",
                                            "7.5 years"))) %>%
  # add names for plotting
  mutate(run = factor(case_when(sample == "7.5 years" ~ "Original",
                                TRUE ~ "Resample"),
                       levels = c("Resample", "Original")))

(fig_c <- ggplot(dat_c_both, aes(x = c_med, y = sample, color = run)) +
    geom_point(size = 5, alpha = 0.75, position = position_dodge(width = -0.5)) +
    geom_errorbarh(aes(xmin = `2.5%`, 
                       xmax = `97.5%`), 
                   height = 0.25,
                   position = position_dodge(width = -0.5)) +
    labs(y = "Length of Time Series", x = "c", color = "Run") +
    scale_color_manual(values = c("grey", "black")) +
    theme_bw())

# Take list above and make into a df.
data_out_04137500_r <- map_df(data_out_04137500_diags, 
                              ~as.data.frame(.x), .id="sample") %>%
  filter(parameter %in% c("r")) %>%
  rename(r_med = `50%`)

# Filter the larger dataset that includes values from the same site.
dat_out_04137500_orig_r <- dat_amax %>%
  filter(site_name == "nwis_04137500")

# Join the two datasets.
dat_out_04137500_orig_r$sample <- "7.5 years"

dat_r_both <- full_join(dat_out_04137500_orig_r, data_out_04137500_r) %>%
  # make norm a factor %>%
  mutate(sample = factor(sample, levels = c("3 months", "6 months",
                                            "1 year", "2 years", "4 years",
                                            "7.5 years"))) %>%
  # add names for plotting
  mutate(run = factor(case_when(sample == "7.5 years" ~ "Original",
                                TRUE ~ "Resample"),
                      levels = c("Resample", "Original")))

(fig_r <- ggplot(dat_r_both, aes(x = r_med, y = sample, color = run)) +
    geom_point(size = 5, alpha = 0.75, position = position_dodge(width = -0.5)) +
    geom_errorbarh(aes(xmin = `2.5%`, 
                       xmax = `97.5%`), 
                   height = 0.25,
                   position = position_dodge(width = -0.5)) +
    labs(y = "Length of Time Series", x = "r", color = "Run") +
    scale_color_manual(values = c("grey", "black")) +
    theme_bw() +
    theme(legend.position = "none"))

# Combine into a single figure.
(fig_r_and_c <- fig_r + fig_c)

# And export.
# ggsave(fig_r_and_c,
#        filename = "figures/beartooth_spring23/r_and_c_ts_length_04137500_fig.jpg",
#        width = 20,
#        height = 10,
#        units = "cm")

#### Reviewer 2 ####

##### Less constrained priors 6 site example #####

# These same 6 sites are also used to answer other reviewer comments above & below.

dat_in6.2 <- dat_in %>%
  filter(site_name %in% c("nwis_06795500", "nwis_02217643", 
                          "nwis_05082500", "nwis_04176500", 
                          "nwis_06893350", "nwis_07075250"))

# Split into a list by ID
l6.2 <- split(dat_in6.2, dat_in6.2$site_name)

# Export dataset for future use.
saveRDS(l6.2, "data_working/list_6sites_Qmaxnorm_SavoySL.rds")

# Rename source data
df2 <- l6.2

# Stan data prep
rstan_options(auto_write=TRUE)
# Specify number of cores
options(mc.cores=6)

# Compile necessary data
stan_data_compile <- function(x){
  data <- list(Ndays=length(x$GPP), # number of days/records
               light = x$light_rel, # relativized light (to maximum light)
               GPP = x$GPP,         # GPP estimates
               GPP_sd = x$GPP_sd,   # standard deviation of GPP estimates
               tQ = x$Q_rel,        # relativized discharge (to Qmax)
               new_e = x$new_e)     # instances of re-initialization needed
  
  return(data)
}

stan_data_l2 <- lapply(df2, function(x) stan_data_compile(x))

# Latent Biomass (Ricker population) Model
# sets initial values to help chains converge
init_Ricker2 <- function(...) {
  list(c = 0.5, s = 1.5) # values updated to match Blaszczak et al.
}

# export results
PM_outputlist_Ricker2 <- lapply(stan_data_l2,
                               function(x) stan("code/beartooth_spring23/Stan_ProductivityModel_lessinformedpriors_R1.stan",
                                                data = x, 
                                                chains = 3,
                                                iter = 5000,
                                                init = init_Ricker2,
                                                control = list(max_treedepth = 12)))

# saveRDS(PM_outputlist_Ricker2, "data_pinyon/pinyon_6rivers_output_2023_07_13.rds")

# Extract only r and c data from the model
# And use a new function to calculate median and 95%tiles.
# Obtain summary statistics using new "extract_summary" function.
# Note, this pulls diagnostics for site-level parameters.
extract_summary2 <- function(x){
  df <- x
  df1 <- summary(df,
                 pars = c("r", "c"),
                 probs = c(0.025, 0.5, 0.975))$summary # 2.5% and 97.5% percentiles
  as.data.frame(df1) %>% rownames_to_column("parameter")
}

# And now map this to the output list. 
data_out6_diags2 <- map(PM_outputlist_Ricker2, extract_summary2)

# Save this out too
saveRDS(data_out6_diags2,
       file = "data_working/pinyon_6rivers_model_rc_params_diags_071323.rds")

# Take list above and make into a df.
data_out6_diags2_df <- map_df(data_out6_diags2, ~as.data.frame(.x), .id="site_name")

dat_out6_ronly <- data_out6_diags2_df %>%
  filter(parameter == "r") %>%
  rename(r_med = `50%`)

dat_out6_conly <- data_out6_diags2_df %>%
  filter(parameter == "c") %>%
  rename(c_med = `50%`)

# plot r values #
# Filter the larger dataset.
dat_amax_6 <- dat_amax %>%
  filter(site_name %in% c("nwis_06795500", "nwis_02217643", 
                          "nwis_05082500", "nwis_04176500", 
                          "nwis_06893350", "nwis_07075250"))

# Join the two datasets.
dat_amax_6$Priors <- "Lowman et al."
dat_out6_ronly$Priors <- "Blaszczak et al."

dat_r_both_6 <- full_join(dat_amax_6, dat_out6_ronly) %>%
  # make priors a factor %>%
  mutate(Priors = factor(Priors, levels = c("Lowman et al.", "Blaszczak et al."))) %>%
  # reorder sites and add names for plotting
  mutate(Site = factor(case_when(site_name == "nwis_02217643" ~ "Parks Creek, GA",
                                 site_name == "nwis_06893350" ~ "Tomahawk Creek, KS",
                                 site_name == "nwis_07075250" ~ "S. Fork Little Red River, AR",
                                 site_name == "nwis_05082500" ~ "Red River, ND",
                                 site_name == "nwis_04176500" ~ "River Raisin, MI",
                                 site_name == "nwis_06795500" ~ "Shell Creek, NE"),
                       levels = c("S. Fork Little Red River, AR", "Tomahawk Creek, KS", 
                                  "River Raisin, MI", "Red River, ND", 
                                  "Parks Creek, GA", "Shell Creek, NE")))

(fig_r <- ggplot(dat_r_both_6, aes(x = r_med, y = Site, color = Priors)) +
    geom_point(size = 5, alpha = 0.75, position = position_dodge(width = -0.5)) +
    geom_errorbarh(aes(xmin = `2.5%`, 
                       xmax = `97.5%`), 
                   height = 0.25,
                   position = position_dodge(width = -0.5)) +
    labs(y = "Site", x = "rmax") +
    scale_color_manual(values = c("black", "grey")) +
    theme_bw()) # Lowman r values slightly lower and more constrained than Blaszczak ones

# plot c values #
# Filter the larger dataset.
dat_Qc_all_6 <- dat_Qc_all %>%
  filter(site_name %in% c("nwis_06795500", "nwis_02217643", 
                          "nwis_05082500", "nwis_04176500", 
                          "nwis_06893350", "nwis_07075250"))

# Join the two datasets.
dat_Qc_all_6$Priors <- "Lowman et al."
dat_out6_conly$Priors <- "Blaszczak et al."

dat_c_both_6 <- full_join(dat_Qc_all_6, dat_out6_conly) %>%
  # make priors a factor %>%
  mutate(Priors = factor(Priors, levels = c("Lowman et al.", "Blaszczak et al."))) %>%
  # reorder sites and add names for plotting
  mutate(Site = factor(case_when(site_name == "nwis_02217643" ~ "Parks Creek, GA",
                                 site_name == "nwis_06893350" ~ "Tomahawk Creek, KS",
                                 site_name == "nwis_07075250" ~ "S. Fork Little Red River, AR",
                                 site_name == "nwis_05082500" ~ "Red River, ND",
                                 site_name == "nwis_04176500" ~ "River Raisin, MI",
                                 site_name == "nwis_06795500" ~ "Shell Creek, NE"),
                       levels = c("S. Fork Little Red River, AR", "Tomahawk Creek, KS", 
                                  "River Raisin, MI", "Red River, ND", 
                                  "Parks Creek, GA", "Shell Creek, NE")))

(fig_c <- ggplot(dat_c_both_6, aes(x = c_med, y = Site, color = Priors)) +
    geom_point(size = 5, alpha = 0.75, position = position_dodge(width = -0.5)) +
    geom_errorbarh(aes(xmin = `2.5%`, 
                       xmax = `97.5%`), 
                   height = 0.25,
                   position = position_dodge(width = -0.5)) +
    labs(y = "Site", x = "c") +
    scale_color_manual(values = c("black", "grey")) +
    theme_bw()) # c values the same, save the AR site that converges poorly

##### c vs. s values 6 site example #####

# Using the data imported above, select for 6 sites of interest that span
# a gradient in coefficients of variation in discharge and storm events.

storm_sites6 <- c("nwis_06795500", "nwis_02217643", "nwis_05082500", "nwis_04176500", "nwis_06893350", "nwis_07075250")

dat_out_storm6 <- dat_out_df %>%
  filter(site_name %in% storm_sites6)

(fig_sc1 <- ggplot(dat_out_storm6 %>%
                   filter(site_name == "nwis_06795500"), aes(x = s, y = c)) +
  geom_point(alpha = 0.75) +
  labs(title = "Shell Creek, NE") +
  #xlim(0, 6) +
  #ylim(0, 2) +
  theme_bw())

(fig_sc2 <- ggplot(dat_out_storm6 %>%
                     filter(site_name == "nwis_02217643"), aes(x = s, y = c)) +
    geom_point(alpha = 0.75) +
    labs(title = "Parks Creek, GA") +
    #xlim(0, 6) +
    #ylim(0, 2) +
    theme_bw())

(fig_sc3 <- ggplot(dat_out_storm6 %>%
                     filter(site_name == "nwis_05082500"), aes(x = s, y = c)) +
    geom_point(alpha = 0.75) +
    labs(title = "Red River, ND") +
    #xlim(0, 6) +
    #ylim(0, 2) +
    theme_bw())

(fig_sc4 <- ggplot(dat_out_storm6 %>%
                     filter(site_name == "nwis_04176500"), aes(x = s, y = c)) +
    geom_point(alpha = 0.75) +
    labs(title = "River Raisin, MI") +
    #xlim(0, 6) +
    #ylim(0, 2) +
    theme_bw())

(fig_sc5 <- ggplot(dat_out_storm6 %>%
                     filter(site_name == "nwis_06893350"), aes(x = s, y = c)) +
    geom_point(alpha = 0.75) +
    labs(title = "Tomahawk Creek, KS") +
    #xlim(0, 6) +
    #ylim(0, 2) +
    theme_bw())

(fig_sc6 <- ggplot(dat_out_storm6 %>%
                     filter(site_name == "nwis_07075250"), aes(x = s, y = c)) +
    geom_point(alpha = 0.75) +
    labs(title = "S. Fork Little Red River, AR") + 
    #xlim(0, 6) +
    #ylim(0, 2) +
    theme_bw())

# Now, let's combine the above using patchwork.
(fig_sc <- (fig_sc1 + fig_sc2 + fig_sc3 +
              fig_sc4 + fig_sc5 + fig_sc6) +
    plot_layout(nrow = 2) +
    plot_annotation(tag_levels = 'A'))

# And export.
# ggsave(fig_sc,
#        filename = "figures/beartooth_spring23/s_vs_c_6sites_071323.jpg",
#        width = 22,
#        height = 16,
#        units = "cm")

# Also, performing a regression on c and s values to summarize how many
# sites display co-variance/potential for equifinality issues and adding
# this language to the Results section briefly.

# Take list containing all iterations of parameters and make into a df.
dat_out_df <- map_df(dat_out, ~as.data.frame(.x), .id="site_name")

dat_out_lnfits <- dat_out_df %>%
  group_by(site_name) %>%
  # fit exponential regression for each site
  #and extract only r-squared values
  summarize(lnfit_r2 = summary(lm(log(c) ~ s))$r.squared) %>%
  ungroup()

hist(dat_out_lnfits$lnfit_r2)

under_0.4_lnfits <- dat_out_lnfits %>%
  filter(lnfit_r2 < 0.4) # 149 of 181 have an R^2 less than 0.4

# End of script.
